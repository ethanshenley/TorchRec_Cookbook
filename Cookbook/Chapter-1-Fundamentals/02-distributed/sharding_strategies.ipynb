{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchRec Sharding Strategies Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrec\n",
    "import torch.distributed as dist\n",
    "from torchrec.distributed.types import ShardingType, ParameterConstraints\n",
    "from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n",
    "from torchrec.distributed.model_parallel import DistributedModelParallel\n",
    "from utils.debugging import TorchRecDebugger\n",
    "from utils.visualization import TorchRecVisualizer\n",
    "from utils.benchmark import TorchRecBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Different Sharding Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_tables():\n",
    "    \"\"\"Create sample tables for demonstration\"\"\"\n",
    "    return [\n",
    "        torchrec.EmbeddingBagConfig(\n",
    "            name=\"large_table\",\n",
    "            embedding_dim=128,\n",
    "            num_embeddings=1_000_000,\n",
    "            feature_names=[\"large_features\"],\n",
    "        ),\n",
    "        torchrec.EmbeddingBagConfig(\n",
    "            name=\"wide_table\",\n",
    "            embedding_dim=256,\n",
    "            num_embeddings=100_000,\n",
    "            feature_names=[\"wide_features\"],\n",
    "        ),\n",
    "        torchrec.EmbeddingBagConfig(\n",
    "            name=\"small_table\",\n",
    "            embedding_dim=32,\n",
    "            num_embeddings=10_000,\n",
    "            feature_names=[\"small_features\"],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "# Create base model\n",
    "base_model = torchrec.EmbeddingBagCollection(\n",
    "    tables=create_sample_tables(),\n",
    "    device=torch.device(\"meta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharding Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharding_types = {\n",
    "    \"TABLE_WISE\": {\n",
    "        \"description\": \"Entire table on one device\",\n",
    "        \"best_for\": \"Small to medium tables\",\n",
    "        \"trade_offs\": {\n",
    "            \"pros\": [\"Low communication overhead\", \"Simple implementation\"],\n",
    "            \"cons\": [\"Limited by single GPU memory\", \"Potential load imbalance\"]\n",
    "        }\n",
    "    },\n",
    "    \"ROW_WISE\": {\n",
    "        \"description\": \"Split tables by rows across devices\",\n",
    "        \"best_for\": \"Tables with many embeddings\",\n",
    "        \"trade_offs\": {\n",
    "            \"pros\": [\"Scales with embedding count\", \"Good memory distribution\"],\n",
    "            \"cons\": [\"All-to-all communication\", \"Complex lookup patterns\"]\n",
    "        }\n",
    "    },\n",
    "    \"COLUMN_WISE\": {\n",
    "        \"description\": \"Split embedding dimensions across devices\",\n",
    "        \"best_for\": \"Tables with large embedding dimensions\",\n",
    "        \"trade_offs\": {\n",
    "            \"pros\": [\"Scales with embedding dim\", \"Balanced computation\"],\n",
    "            \"cons\": [\"All-to-all communication\", \"Complex reduction\"]\n",
    "        }\n",
    "    },\n",
    "    \"DATA_PARALLEL\": {\n",
    "        \"description\": \"Full table replica on each device\",\n",
    "        \"best_for\": \"Small tables with high lookup frequency\",\n",
    "        \"trade_offs\": {\n",
    "            \"pros\": [\"Fast forward pass\", \"Simple implementation\"],\n",
    "            \"cons\": [\"High memory usage\", \"Gradient synchronization overhead\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(sharding_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Different Sharding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sharding_configs():\n",
    "    \"\"\"Create different sharding configurations\"\"\"\n",
    "    configs = {}\n",
    "    \n",
    "    # Table-wise sharding\n",
    "    configs[\"table_wise\"] = {\n",
    "        \"large_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.TABLE_WISE.value]\n",
    "        ),\n",
    "        \"wide_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.TABLE_WISE.value]\n",
    "        ),\n",
    "        \"small_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.TABLE_WISE.value]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Row-wise sharding\n",
    "    configs[\"row_wise\"] = {\n",
    "        \"large_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.ROW_WISE.value]\n",
    "        ),\n",
    "        \"wide_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.ROW_WISE.value]\n",
    "        ),\n",
    "        \"small_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.ROW_WISE.value]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Column-wise sharding\n",
    "    configs[\"column_wise\"] = {\n",
    "        \"large_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.COLUMN_WISE.value]\n",
    "        ),\n",
    "        \"wide_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.COLUMN_WISE.value]\n",
    "        ),\n",
    "        \"small_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.COLUMN_WISE.value]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Mixed sharding (realistic scenario)\n",
    "    configs[\"mixed\"] = {\n",
    "        \"large_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.ROW_WISE.value]\n",
    "        ),\n",
    "        \"wide_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.COLUMN_WISE.value]\n",
    "        ),\n",
    "        \"small_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.TABLE_WISE.value]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Different Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShardingBenchmark:\n",
    "    def __init__(self, world_size, device=\"cuda\"):\n",
    "        self.world_size = world_size\n",
    "        self.device = device\n",
    "        self.debugger = TorchRecDebugger()\n",
    "        self.benchmark = TorchRecBenchmark()\n",
    "        \n",
    "    def create_sharded_model(self, base_model, sharding_config):\n",
    "        \"\"\"Create sharded model with specific configuration\"\"\"\n",
    "        topology = Topology(\n",
    "            world_size=self.world_size,\n",
    "            compute_device=self.device\n",
    "        )\n",
    "        \n",
    "        planner = EmbeddingShardingPlanner(\n",
    "            topology=topology,\n",
    "            constraints=sharding_config\n",
    "        )\n",
    "        \n",
    "        plan = planner.collective_plan(\n",
    "            base_model,\n",
    "            [torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder()]\n",
    "        )\n",
    "        \n",
    "        return DistributedModelParallel(\n",
    "            module=base_model,\n",
    "            plan=plan,\n",
    "            device=torch.device(self.device)\n",
    "        )\n",
    "    \n",
    "    def generate_sample_batch(self, batch_size=32):\n",
    "        \"\"\"Generate sample batch for testing\"\"\"\n",
    "        values = torch.randint(0, 1000, (batch_size * 10,))\n",
    "        lengths = torch.ones(batch_size) * 10\n",
    "        \n",
    "        return torchrec.sparse.jagged_tensor.KeyedJaggedTensor.from_lengths_sync(\n",
    "            keys=[\"large_features\", \"wide_features\", \"small_features\"],\n",
    "            values=values.to(self.device),\n",
    "            lengths=lengths\n",
    "        )\n",
    "    \n",
    "    def benchmark_strategy(self, model, batch):\n",
    "        \"\"\"Benchmark specific sharding strategy\"\"\"\n",
    "        results = self.benchmark.benchmark_forward(model, batch, batch_size=32)\n",
    "        memory_stats = self.debugger.memory_status()\n",
    "        \n",
    "        return {\n",
    "            \"batch_time_ms\": results.batch_time_ms,\n",
    "            \"throughput\": results.throughput,\n",
    "            \"memory_gb\": memory_stats[\"allocated\"] / 1e9\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sharding_comparison(world_size=2):\n",
    "    \"\"\"Compare different sharding strategies\"\"\"\n",
    "    benchmark = ShardingBenchmark(world_size)\n",
    "    configs = create_sharding_configs()\n",
    "    results = {}\n",
    "    \n",
    "    for strategy_name, config in configs.items():\n",
    "        print(f\"\\nTesting {strategy_name} sharding strategy...\")\n",
    "        \n",
    "        # Create sharded model\n",
    "        model = benchmark.create_sharded_model(base_model, config)\n",
    "        \n",
    "        # Generate test batch\n",
    "        batch = benchmark.generate_sample_batch()\n",
    "        \n",
    "        # Run benchmark\n",
    "        results[strategy_name] = benchmark.benchmark_strategy(model, batch)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(results):\n",
    "    \"\"\"Visualize benchmark results\"\"\"\n",
    "    visualizer = TorchRecVisualizer()\n",
    "    \n",
    "    # Plot latency comparison\n",
    "    latencies = [res[\"batch_time_ms\"] for res in results.values()]\n",
    "    strategies = list(results.keys())\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(strategies, latencies)\n",
    "    plt.title(\"Latency by Sharding Strategy\")\n",
    "    plt.ylabel(\"Batch Time (ms)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot memory usage\n",
    "    memory_usage = [res[\"memory_gb\"] for res in results.values()]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(strategies, memory_usage)\n",
    "    plt.title(\"Memory Usage by Sharding Strategy\")\n",
    "    plt.ylabel(\"Memory (GB)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharding_guidelines = {\n",
    "    \"Table Size Based\": {\n",
    "        \"Large Tables (>1M rows)\": \"Consider ROW_WISE sharding\",\n",
    "        \"Wide Tables (>256 dim)\": \"Consider COLUMN_WISE sharding\",\n",
    "        \"Small Tables (<100K rows)\": \"Consider TABLE_WISE or DATA_PARALLEL\"\n",
    "    },\n",
    "    \"Access Pattern Based\": {\n",
    "        \"High Frequency Access\": \"Prefer DATA_PARALLEL or TABLE_WISE\",\n",
    "        \"Sparse Access\": \"ROW_WISE can be more efficient\",\n",
    "        \"Mixed Access\": \"Consider mixed sharding strategy\"\n",
    "    },\n",
    "    \"Hardware Considerations\": {\n",
    "        \"Limited GPU Memory\": \"Prefer ROW_WISE or COLUMN_WISE\",\n",
    "        \"Fast GPU Interconnect\": \"All-to-all communication less problematic\",\n",
    "        \"Multiple Nodes\": \"Consider communication overhead carefully\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(sharding_guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_sharding_plan(tables_info):\n",
    "    \"\"\"Create sharding plan based on table characteristics\"\"\"\n",
    "    constraints = {}\n",
    "    \n",
    "    for table_name, info in tables_info.items():\n",
    "        if info[\"num_embeddings\"] > 1_000_000:\n",
    "            # Large tables get row-wise sharding\n",
    "            constraints[table_name] = ParameterConstraints(\n",
    "                sharding_types=[ShardingType.ROW_WISE.value]\n",
    "            )\n",
    "        elif info[\"embedding_dim\"] > 256:\n",
    "            # Wide tables get column-wise sharding\n",
    "            constraints[table_name] = ParameterConstraints(\n",
    "                sharding_types=[ShardingType.COLUMN_WISE.value]\n",
    "            )\n",
    "        else:\n",
    "            # Small tables get table-wise sharding\n",
    "            constraints[table_name] = ParameterConstraints(\n",
    "                sharding_types=[ShardingType.TABLE_WISE.value]\n",
    "            )\n",
    "    \n",
    "    return constraints\n",
    "\n",
    "# Example usage\n",
    "tables_info = {\n",
    "    \"large_table\": {\"num_embeddings\": 1_500_000, \"embedding_dim\": 128},\n",
    "    \"wide_table\": {\"num_embeddings\": 100_000, \"embedding_dim\": 512},\n",
    "    \"small_table\": {\"num_embeddings\": 10_000, \"embedding_dim\": 32}\n",
    "}\n",
    "\n",
    "production_constraints = create_production_sharding_plan(tables_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sharding_plan(plan):\n",
    "    \"\"\"Analyze sharding plan distribution\"\"\"\n",
    "    analysis = {\n",
    "        \"sharding_types\": {},\n",
    "        \"memory_distribution\": {},\n",
    "        \"communication_patterns\": {}\n",
    "    }\n",
    "    \n",
    "    for table_name, sharding in plan.items():\n",
    "        # Analyze sharding type distribution\n",
    "        stype = sharding.sharding_type\n",
    "        analysis[\"sharding_types\"][stype] = analysis[\"sharding_types\"].get(stype, 0) + 1\n",
    "        \n",
    "        # Analyze memory distribution\n",
    "        for shard in sharding.sharding_spec.shards:\n",
    "            device = shard.placement\n",
    "            memory = shard.shard_sizes[0] * shard.shard_sizes[1] * 4  # float32\n",
    "            analysis[\"memory_distribution\"][device] = \\\n",
    "                analysis[\"memory_distribution\"].get(device, 0) + memory\n",
    "    \n",
    "    return analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
