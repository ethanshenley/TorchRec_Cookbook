{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Node Training with TorchRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchrec\n",
    "import torch.distributed as dist\n",
    "from torchrec.distributed.model_parallel import DistributedModelParallel\n",
    "from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n",
    "from torchrec.distributed.types import ShardingType, ParameterConstraints, ShardingEnv\n",
    "from utils.debugging import TorchRecDebugger\n",
    "from utils.benchmark import TorchRecBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Node Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNodeConfig:\n",
    "    \"\"\"Configuration for multi-node setup\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        world_size: int,\n",
    "        num_nodes: int,\n",
    "        node_rank: int,\n",
    "        master_addr: str = \"localhost\",\n",
    "        master_port: str = \"29500\"\n",
    "    ):\n",
    "        self.world_size = world_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.node_rank = node_rank\n",
    "        self.gpus_per_node = world_size // num_nodes\n",
    "        self.master_addr = master_addr\n",
    "        self.master_port = master_port\n",
    "\n",
    "def setup_multi_node(config: MultiNodeConfig, local_rank: int):\n",
    "    \"\"\"Setup multi-node distributed environment\"\"\"\n",
    "    # Calculate global rank\n",
    "    global_rank = config.node_rank * config.gpus_per_node + local_rank\n",
    "    \n",
    "    # Set environment variables\n",
    "    os.environ[\"MASTER_ADDR\"] = config.master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = config.master_port\n",
    "    os.environ[\"WORLD_SIZE\"] = str(config.world_size)\n",
    "    os.environ[\"RANK\"] = str(global_rank)\n",
    "    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n",
    "    \n",
    "    # Initialize process group\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    return global_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Node Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_model():\n",
    "    \"\"\"Create a large model that benefits from multi-node training\"\"\"\n",
    "    tables = [\n",
    "        torchrec.EmbeddingBagConfig(\n",
    "            name=\"large_sparse_table\",\n",
    "            embedding_dim=128,\n",
    "            num_embeddings=10_000_000,  # 10M rows\n",
    "            feature_names=[\"large_sparse_features\"],\n",
    "        ),\n",
    "        torchrec.EmbeddingBagConfig(\n",
    "            name=\"dense_feature_table\",\n",
    "            embedding_dim=256,\n",
    "            num_embeddings=1_000_000,  # 1M rows\n",
    "            feature_names=[\"dense_features\"],\n",
    "        ),\n",
    "        torchrec.EmbeddingBagConfig(\n",
    "            name=\"shared_table\",\n",
    "            embedding_dim=64,\n",
    "            num_embeddings=5_000_000,  # 5M rows\n",
    "            feature_names=[\"feature1\", \"feature2\"],\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    return torchrec.EmbeddingBagCollection(\n",
    "        tables=tables,\n",
    "        device=torch.device(\"meta\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Node Sharding Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sharding_plan(model, config: MultiNodeConfig):\n",
    "    \"\"\"Create sharding plan optimized for multi-node setup\"\"\"\n",
    "    \n",
    "    # Define constraints based on table sizes\n",
    "    constraints = {\n",
    "        \"large_sparse_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.ROW_WISE.value]\n",
    "        ),\n",
    "        \"dense_feature_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.COLUMN_WISE.value]\n",
    "        ),\n",
    "        \"shared_table\": ParameterConstraints(\n",
    "            sharding_types=[ShardingType.TABLE_ROW_WISE.value]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Create topology considering multiple nodes\n",
    "    topology = Topology(\n",
    "        world_size=config.world_size,\n",
    "        compute_device=\"cuda\",\n",
    "        local_world_size=config.gpus_per_node\n",
    "    )\n",
    "    \n",
    "    # Create planner\n",
    "    planner = EmbeddingShardingPlanner(\n",
    "        topology=topology,\n",
    "        constraints=constraints\n",
    "    )\n",
    "    \n",
    "    # Generate plan\n",
    "    return planner.collective_plan(\n",
    "        model,\n",
    "        [torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management Across Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNodeDataManager:\n",
    "    \"\"\"Manage data distribution across nodes\"\"\"\n",
    "    def __init__(self, config: MultiNodeConfig, rank: int):\n",
    "        self.config = config\n",
    "        self.rank = rank\n",
    "        self.debugger = TorchRecDebugger()\n",
    "    \n",
    "    def generate_node_specific_batch(self, batch_size: int):\n",
    "        \"\"\"Generate data specific to this node/rank\"\"\"\n",
    "        # Calculate local batch size\n",
    "        local_batch_size = batch_size // self.config.world_size\n",
    "        \n",
    "        # Generate values with rank-specific offset\n",
    "        values = torch.randint(\n",
    "            low=self.rank * 1000,\n",
    "            high=(self.rank + 1) * 1000,\n",
    "            size=(local_batch_size * 10,)\n",
    "        )\n",
    "        \n",
    "        lengths = torch.ones(local_batch_size) * 10\n",
    "        \n",
    "        # Create KJT\n",
    "        return torchrec.sparse.jagged_tensor.KeyedJaggedTensor.from_lengths_sync(\n",
    "            keys=[\"large_sparse_features\", \"dense_features\", \"feature1\", \"feature2\"],\n",
    "            values=values.cuda(),\n",
    "            lengths=lengths.repeat(4)  # 4 features\n",
    "        )\n",
    "    \n",
    "    def verify_data_distribution(self, batch):\n",
    "        \"\"\"Verify data is correctly distributed\"\"\"\n",
    "        return {\n",
    "            \"local_batch_size\": len(batch.lengths()) // 4,  # 4 features\n",
    "            \"value_range\": (batch.values().min().item(), \n",
    "                          batch.values().max().item()),\n",
    "            \"device\": batch.values().device\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multi-Node Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNodeTrainer:\n",
    "    \"\"\"Manage distributed training across nodes\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: DistributedModelParallel,\n",
    "        config: MultiNodeConfig,\n",
    "        rank: int\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.rank = rank\n",
    "        self.optimizer = torch.optim.Adam(model.parameters())\n",
    "        self.data_manager = MultiNodeDataManager(config, rank)\n",
    "        self.benchmark = TorchRecBenchmark()\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(batch)\n",
    "        embeddings = output.wait()  # Wait for async computation\n",
    "        \n",
    "        # Simple loss for demonstration\n",
    "        loss = torch.mean(embeddings.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step optimizer\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_epoch(self, num_batches, batch_size):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        losses = []\n",
    "        timings = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            # Generate batch\n",
    "            batch = self.data_manager.generate_node_specific_batch(batch_size)\n",
    "            \n",
    "            # Time the training step\n",
    "            start_time = torch.cuda.Event(enable_timing=True)\n",
    "            end_time = torch.cuda.Event(enable_timing=True)\n",
    "            \n",
    "            start_time.record()\n",
    "            loss = self.train_step(batch)\n",
    "            end_time.record()\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            timings.append(start_time.elapsed_time(end_time))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if self.rank == 0 and i % 10 == 0:\n",
    "                print(f\"Batch {i}, Loss: {loss:.4f}, \"\n",
    "                      f\"Time: {timings[-1]:.2f}ms\")\n",
    "        \n",
    "        return losses, timings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Node Communication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunicationAnalyzer:\n",
    "    \"\"\"Analyze communication patterns between nodes\"\"\"\n",
    "    def __init__(self, config: MultiNodeConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def analyze_communication(self, model: DistributedModelParallel):\n",
    "        \"\"\"Analyze communication patterns in the model\"\"\"\n",
    "        communication_stats = {\n",
    "            \"all_to_all\": 0,\n",
    "            \"reduce_scatter\": 0,\n",
    "            \"all_gather\": 0\n",
    "        }\n",
    "        \n",
    "        # Analyze sharding plan\n",
    "        for name, param in model.named_parameters():\n",
    "            if hasattr(param, 'linked_param'):\n",
    "                if 'row_wise' in str(param.linked_param):\n",
    "                    communication_stats['all_to_all'] += 1\n",
    "                elif 'column_wise' in str(param.linked_param):\n",
    "                    communication_stats['reduce_scatter'] += 1\n",
    "                    communication_stats['all_gather'] += 1\n",
    "        \n",
    "        return communication_stats\n",
    "    \n",
    "    def estimate_communication_volume(self, model: DistributedModelParallel):\n",
    "        \"\"\"Estimate communication volume between nodes\"\"\"\n",
    "        total_bytes = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if hasattr(param, 'linked_param'):\n",
    "                # Estimate bytes transferred\n",
    "                total_bytes += param.numel() * param.element_size()\n",
    "        \n",
    "        return {\n",
    "            \"total_gb\": total_bytes / 1e9,\n",
    "            \"gb_per_node\": total_bytes / (1e9 * self.config.num_nodes)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNodeMonitor:\n",
    "    \"\"\"Monitor multi-node training performance\"\"\"\n",
    "    def __init__(self, config: MultiNodeConfig, rank: int):\n",
    "        self.config = config\n",
    "        self.rank = rank\n",
    "        self.debugger = TorchRecDebugger()\n",
    "    \n",
    "    def monitor_step(self, model, batch):\n",
    "        \"\"\"Monitor single step performance\"\"\"\n",
    "        memory_stats = self.debugger.memory_status()\n",
    "        \n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"node\": self.rank // self.config.gpus_per_node,\n",
    "            \"local_rank\": self.rank % self.config.gpus_per_node,\n",
    "            \"memory_allocated_gb\": memory_stats[\"allocated\"] / 1e9,\n",
    "            \"memory_reserved_gb\": memory_stats[\"reserved\"] / 1e9\n",
    "        }\n",
    "    \n",
    "    def collect_global_stats(self, local_stats):\n",
    "        \"\"\"Collect stats from all nodes\"\"\"\n",
    "        # Gather stats from all ranks\n",
    "        all_stats = [None] * self.config.world_size\n",
    "        dist.all_gather_object(all_stats, local_stats)\n",
    "        \n",
    "        return all_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Multi-Node Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(local_rank, config: MultiNodeConfig):\n",
    "    \"\"\"Main worker function for each process\"\"\"\n",
    "    # Setup distributed\n",
    "    rank = setup_multi_node(config, local_rank)\n",
    "    \n",
    "    # Create model\n",
    "    base_model = create_large_model()\n",
    "    plan = create_sharding_plan(base_model, config)\n",
    "    \n",
    "    model = DistributedModelParallel(\n",
    "        module=base_model,\n",
    "        device=torch.device(f\"cuda:{local_rank}\"),\n",
    "        plan=plan\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = MultiNodeTrainer(model, config, rank)\n",
    "    monitor = MultiNodeMonitor(config, rank)\n",
    "    analyzer = CommunicationAnalyzer(config)\n",
    "    \n",
    "    # Train\n",
    "    batch_size = 1024\n",
    "    num_batches = 100\n",
    "    \n",
    "    losses, timings = trainer.train_epoch(num_batches, batch_size)\n",
    "    \n",
    "    # Collect stats\n",
    "    local_stats = monitor.monitor_step(model, None)\n",
    "    all_stats = monitor.collect_global_stats(local_stats)\n",
    "    \n",
    "    # Analysis (on rank 0)\n",
    "    if rank == 0:\n",
    "        comm_stats = analyzer.analyze_communication(model)\n",
    "        comm_volume = analyzer.estimate_communication_volume(model)\n",
    "        \n",
    "        print(\"\\nTraining Summary:\")\n",
    "        print(f\"Average Loss: {sum(losses) / len(losses):.4f}\")\n",
    "        print(f\"Average Batch Time: {sum(timings) / len(timings):.2f}ms\")\n",
    "        print(\"\\nCommunication Analysis:\")\n",
    "        print(f\"Communication Patterns: {comm_stats}\")\n",
    "        print(f\"Communication Volume: {comm_volume}\")\n",
    "        print(\"\\nNode Statistics:\")\n",
    "        for stats in all_stats:\n",
    "            print(f\"Node {stats['node']}, Rank {stats['rank']}: \"\n",
    "                  f\"{stats['memory_allocated_gb']:.2f}GB allocated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_multi_node(num_nodes, gpus_per_node):\n",
    "    \"\"\"Launch multi-node training\"\"\"\n",
    "    config = MultiNodeConfig(\n",
    "        world_size=num_nodes * gpus_per_node,\n",
    "        num_nodes=num_nodes,\n",
    "        node_rank=int(os.environ.get(\"NODE_RANK\", 0))\n",
    "    )\n",
    "    \n",
    "    torch.multiprocessing.spawn(\n",
    "        main_worker,\n",
    "        args=(config,),\n",
    "        nprocs=gpus_per_node\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
