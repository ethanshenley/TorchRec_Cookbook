{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Implementation with Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrec\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from typing import Dict, List, Tuple, Optional, NamedTuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Processing Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SemanticSearchResult:\n",
    "    \"\"\"Container for semantic search results\"\"\"\n",
    "    document_id: int\n",
    "    score: float\n",
    "    semantic_similarity: float\n",
    "    entity_match_score: float\n",
    "    intent_match_score: float\n",
    "    text_snippet: str\n",
    "    matched_entities: List[str]\n",
    "    relevance_explanation: str\n",
    "\n",
    "class SemanticProcessor:\n",
    "    \"\"\"Process queries and documents for semantic understanding\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        t5_model: str = \"t5-base\",\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.device = device\n",
    "        \n",
    "        # Semantic embedding model\n",
    "        self.embedding_model = SentenceTransformer(model_name).to(device)\n",
    "        \n",
    "        # Query understanding model\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model).to(device)\n",
    "        \n",
    "        # Entity recognition model\n",
    "        self.entity_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "        self.entity_model = AutoModel.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\").to(device)\n",
    "    \n",
    "    def process_query(self, query: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Process query for semantic search\"\"\"\n",
    "        # Generate semantic embedding\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Understand query intent\n",
    "        intent_input = f\"classify intent: {query}\"\n",
    "        intent_tokens = self.tokenizer(\n",
    "            intent_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        intent_output = self.t5_model.generate(\n",
    "            **intent_tokens,\n",
    "            max_length=32,\n",
    "            num_beams=4\n",
    "        )\n",
    "        intent = self.tokenizer.decode(intent_output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract entities\n",
    "        entity_tokens = self.entity_tokenizer(\n",
    "            query,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        entity_outputs = self.entity_model(**entity_tokens)\n",
    "        entity_embeddings = entity_outputs.last_hidden_state\n",
    "        \n",
    "        return {\n",
    "            \"embedding\": query_embedding,\n",
    "            \"intent\": intent,\n",
    "            \"entity_embeddings\": entity_embeddings,\n",
    "            \"original_query\": query\n",
    "        }\n",
    "    \n",
    "    def expand_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate query expansions\"\"\"\n",
    "        expansion_input = f\"expand query: {query}\"\n",
    "        expansion_tokens = self.tokenizer(\n",
    "            expansion_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        expansion_outputs = self.t5_model.generate(\n",
    "            **expansion_tokens,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            num_return_sequences=3\n",
    "        )\n",
    "        \n",
    "        expansions = [\n",
    "            self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            for output in expansion_outputs\n",
    "        ]\n",
    "        \n",
    "        return expansions\n",
    "\n",
    "class SemanticIndex:\n",
    "    \"\"\"Index for semantic search\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        processor: SemanticProcessor,\n",
    "        use_faiss: bool = True\n",
    "    ):\n",
    "        self.processor = processor\n",
    "        self.use_faiss = use_faiss\n",
    "        self.document_store = {}\n",
    "        self.document_embeddings = None\n",
    "        \n",
    "        if use_faiss:\n",
    "            import faiss\n",
    "            self.index = None\n",
    "    \n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[Dict[str, str]],\n",
    "        batch_size: int = 32\n",
    "    ):\n",
    "        \"\"\"Add documents to semantic index\"\"\"\n",
    "        print(\"Processing documents...\")\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            batch_texts = [doc[\"text\"] for doc in batch]\n",
    "            \n",
    "            # Generate embeddings\n",
    "            batch_embeddings = self.processor.embedding_model.encode(\n",
    "                batch_texts,\n",
    "                convert_to_tensor=True,\n",
    "                device=self.processor.device\n",
    "            )\n",
    "            \n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            \n",
    "            # Process and store documents\n",
    "            for j, doc in enumerate(batch):\n",
    "                doc_id = i + j\n",
    "                processed_doc = {\n",
    "                    \"id\": doc_id,\n",
    "                    \"text\": doc[\"text\"],\n",
    "                    \"entities\": self._extract_entities(doc[\"text\"]),\n",
    "                    \"semantic_properties\": self._extract_semantic_properties(doc[\"text\"])\n",
    "                }\n",
    "                self.document_store[doc_id] = processed_doc\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        self.document_embeddings = np.vstack(embeddings)\n",
    "        \n",
    "        if self.use_faiss:\n",
    "            # Initialize FAISS index\n",
    "            dimension = self.document_embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dimension)\n",
    "            self.index.add(self.document_embeddings)\n",
    "    \n",
    "    def _extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entities from text\"\"\"\n",
    "        tokens = self.processor.entity_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.processor.device)\n",
    "        \n",
    "        outputs = self.processor.entity_model(**tokens)\n",
    "        entity_embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        # Simplified entity extraction (in practice, would use more sophisticated NER)\n",
    "        return []  # Placeholder\n",
    "    \n",
    "    def _extract_semantic_properties(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract semantic properties from text\"\"\"\n",
    "        property_input = f\"extract properties: {text}\"\n",
    "        property_tokens = self.processor.tokenizer(\n",
    "            property_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.processor.device)\n",
    "        \n",
    "        property_output = self.processor.t5_model.generate(\n",
    "            **property_tokens,\n",
    "            max_length=128,\n",
    "            num_beams=4\n",
    "        )\n",
    "        \n",
    "        properties = self.processor.tokenizer.decode(\n",
    "            property_output[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Simplified property extraction (in practice, would parse the output)\n",
    "        return {}  # Placeholder\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 10,\n",
    "        rerank: bool = True\n",
    "    ) -> List[SemanticSearchResult]:\n",
    "        \"\"\"Perform semantic search\"\"\"\n",
    "        # Process query\n",
    "        query_data = self.processor.process_query(query)\n",
    "        query_embedding = query_data[\"embedding\"].cpu().numpy()\n",
    "        \n",
    "        # Initial retrieval\n",
    "        if self.use_faiss:\n",
    "            scores, doc_indices = self.index.search(\n",
    "                query_embedding.reshape(1, -1),\n",
    "                k * 2  # Retrieve more for reranking\n",
    "            )\n",
    "            initial_results = [\n",
    "                (self.document_store[idx], score)\n",
    "                for score, idx in zip(scores[0], doc_indices[0])\n",
    "            ]\n",
    "        else:\n",
    "            # Compute similarities\n",
    "            similarities = cosine_similarity(\n",
    "                query_embedding.reshape(1, -1),\n",
    "                self.document_embeddings\n",
    "            )[0]\n",
    "            \n",
    "            # Get top results\n",
    "            top_indices = np.argsort(similarities)[-k * 2:][::-1]\n",
    "            initial_results = [\n",
    "                (self.document_store[idx], similarities[idx])\n",
    "                for idx in top_indices\n",
    "            ]\n",
    "        \n",
    "        if rerank:\n",
    "            # Rerank results using more sophisticated scoring\n",
    "            results = []\n",
    "            for doc, initial_score in initial_results:\n",
    "                # Calculate additional relevance signals\n",
    "                semantic_score = self._calculate_semantic_score(\n",
    "                    query_data,\n",
    "                    doc\n",
    "                )\n",
    "                \n",
    "                entity_score = self._calculate_entity_score(\n",
    "                    query_data,\n",
    "                    doc\n",
    "                )\n",
    "                \n",
    "                intent_score = self._calculate_intent_score(\n",
    "                    query_data,\n",
    "                    doc\n",
    "                )\n",
    "                \n",
    "                # Combined score\n",
    "                final_score = (\n",
    "                    initial_score * 0.4 +\n",
    "                    semantic_score * 0.3 +\n",
    "                    entity_score * 0.2 +\n",
    "                    intent_score * 0.1\n",
    "                )\n",
    "                \n",
    "                results.append(\n",
    "                    SemanticSearchResult(\n",
    "                        document_id=doc[\"id\"],\n",
    "                        score=final_score,\n",
    "                        semantic_similarity=semantic_score,\n",
    "                        entity_match_score=entity_score,\n",
    "                        intent_match_score=intent_score,\n",
    "                        text_snippet=self._generate_snippet(query, doc[\"text\"]),\n",
    "                        matched_entities=self._find_matching_entities(\n",
    "                            query_data,\n",
    "                            doc\n",
    "                        ),\n",
    "                        relevance_explanation=self._explain_relevance(\n",
    "                            query_data,\n",
    "                            doc,\n",
    "                            final_score\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            # Sort by final score\n",
    "            results.sort(key=lambda x: x.score, reverse=True)\n",
    "            return results[:k]\n",
    "        \n",
    "        return initial_results[:k]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
