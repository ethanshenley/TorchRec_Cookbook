{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding-Based Search with TorchRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrec\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, NamedTuple\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Search Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmbeddingIndex:\n",
    "    \"\"\"Container for embedding index\"\"\"\n",
    "    dimension: int\n",
    "    index: faiss.Index\n",
    "    id_to_metadata: Dict[int, Dict]\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query_vector: np.ndarray,\n",
    "        k: int = 10\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Search for nearest neighbors\"\"\"\n",
    "        return self.index.search(query_vector.reshape(1, -1), k)\n",
    "\n",
    "class DualEncoder(torch.nn.Module):\n",
    "    \"\"\"Encoder for query and document embeddings\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared embedding layer\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Query encoder\n",
    "        self.query_encoder = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Document encoder\n",
    "        self.doc_encoder = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Projection layers\n",
    "        self.query_projection = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, embedding_dim),\n",
    "            torch.nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.doc_projection = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, embedding_dim),\n",
    "            torch.nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def encode_query(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        lengths: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Embed tokens\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        # Pack sequence\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Encode\n",
    "        _, (hidden, _) = self.query_encoder(packed)\n",
    "        \n",
    "        # Combine bidirectional states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Project\n",
    "        return self.query_projection(hidden)\n",
    "    \n",
    "    def encode_document(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        lengths: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Embed tokens\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        # Pack sequence\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Encode\n",
    "        _, (hidden, _) = self.doc_encoder(packed)\n",
    "        \n",
    "        # Combine bidirectional states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Project\n",
    "        return self.doc_projection(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Building and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSearchEngine:\n",
    "    \"\"\"Engine for embedding-based search\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: DualEncoder,\n",
    "        embedding_dim: int,\n",
    "        index_type: str = \"IVFFlat\",\n",
    "        n_cells: int = 100,\n",
    "        n_probes: int = 10,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # Create FAISS index\n",
    "        if index_type == \"IVFFlat\":\n",
    "            quantizer = faiss.IndexFlatIP(embedding_dim)\n",
    "            self.index = faiss.IndexIVFFlat(\n",
    "                quantizer,\n",
    "                embedding_dim,\n",
    "                n_cells,\n",
    "                faiss.METRIC_INNER_PRODUCT\n",
    "            )\n",
    "        elif index_type == \"HNSW\":\n",
    "            self.index = faiss.IndexHNSWFlat(\n",
    "                embedding_dim,\n",
    "                32,  # M parameter for HNSW\n",
    "                faiss.METRIC_INNER_PRODUCT\n",
    "            )\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(embedding_dim)\n",
    "        \n",
    "        self.n_probes = n_probes\n",
    "        self.document_store = {}\n",
    "    \n",
    "    def build_index(\n",
    "        self,\n",
    "        documents: List[Dict],\n",
    "        batch_size: int = 32\n",
    "    ):\n",
    "        \"\"\"Build search index from documents\"\"\"\n",
    "        print(\"Encoding documents...\")\n",
    "        all_embeddings = []\n",
    "        \n",
    "        self.encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch_docs = documents[i:i + batch_size]\n",
    "                \n",
    "                # Prepare batch\n",
    "                tokens = torch.tensor(\n",
    "                    [doc[\"tokens\"] for doc in batch_docs],\n",
    "                    device=self.device\n",
    "                )\n",
    "                lengths = torch.tensor(\n",
    "                    [len(doc[\"tokens\"]) for doc in batch_docs],\n",
    "                    device=self.device\n",
    "                )\n",
    "                \n",
    "                # Encode\n",
    "                embeddings = self.encoder.encode_document(tokens, lengths)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "                \n",
    "                # Store documents\n",
    "                for j, doc in enumerate(batch_docs):\n",
    "                    self.document_store[i + j] = doc\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        faiss.normalize_L2(all_embeddings)\n",
    "        \n",
    "        # Train index if needed\n",
    "        if isinstance(self.index, faiss.IndexIVFFlat):\n",
    "            print(\"Training index...\")\n",
    "            self.index.train(all_embeddings)\n",
    "        \n",
    "        # Add vectors to index\n",
    "        print(\"Adding vectors to index...\")\n",
    "        self.index.add(all_embeddings)\n",
    "        \n",
    "        if hasattr(self.index, 'nprobe'):\n",
    "            self.index.nprobe = self.n_probes\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 10,\n",
    "        return_scores: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        # Tokenize query (simplified)\n",
    "        query_tokens = torch.tensor(\n",
    "            [ord(c) % 100 for c in query],  # Simplified tokenization\n",
    "            device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        query_length = torch.tensor([len(query)], device=self.device)\n",
    "        \n",
    "        # Encode query\n",
    "        self.encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.encoder.encode_query(\n",
    "                query_tokens,\n",
    "                query_length\n",
    "            )\n",
    "        \n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding.cpu().numpy()\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Return results\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx == -1:  # FAISS returns -1 for not enough results\n",
    "                continue\n",
    "                \n",
    "            result = self.document_store[idx].copy()\n",
    "            if return_scores:\n",
    "                result[\"score\"] = float(score)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderTrainer:\n",
    "    \"\"\"Training infrastructure for dual encoder\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: DualEncoder,\n",
    "        learning_rate: float = 0.001,\n",
    "        temperature: float = 0.1,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            encoder.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=1000\n",
    "        )\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        query_tokens: torch.Tensor,\n",
    "        query_lengths: torch.Tensor,\n",
    "        pos_doc_tokens: torch.Tensor,\n",
    "        pos_doc_lengths: torch.Tensor,\n",
    "        neg_doc_tokens: torch.Tensor,\n",
    "        neg_doc_lengths: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Move to device\n",
    "        query_tokens = query_tokens.to(self.device)\n",
    "        query_lengths = query_lengths.to(self.device)\n",
    "        pos_doc_tokens = pos_doc_tokens.to(self.device)\n",
    "        pos_doc_lengths = pos_doc_lengths.to(self.device)\n",
    "        neg_doc_tokens = neg_doc_tokens.to(self.device)\n",
    "        neg_doc_lengths = neg_doc_lengths.to(self.device)\n",
    "        \n",
    "        # Encode queries and documents\n",
    "        query_embeddings = self.encoder.encode_query(\n",
    "            query_tokens,\n",
    "            query_lengths\n",
    "        )\n",
    "        \n",
    "        pos_doc_embeddings = self.encoder.encode_document(\n",
    "            pos_doc_tokens,\n",
    "            pos_doc_lengths\n",
    "        )\n",
    "        \n",
    "        neg_doc_embeddings = self.encoder.encode_document(\n",
    "            neg_doc_tokens,\n",
    "            neg_doc_lengths\n",
    "        )\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_similarities = torch.sum(\n",
    "            query_embeddings * pos_doc_embeddings,\n",
    "            dim=1\n",
    "        ) / self.temperature\n",
    "        \n",
    "        neg_similarities = torch.sum(\n",
    "            query_embeddings * neg_doc_embeddings,\n",
    "            dim=1\n",
    "        ) / self.temperature\n",
    "        \n",
    "        # Compute loss (InfoNCE)\n",
    "        logits = torch.stack([pos_similarities, neg_similarities], dim=1)\n",
    "        labels = torch.zeros(len(logits), device=self.device).long()\n",
    "        \n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = (logits.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"accuracy\": accuracy.item(),\n",
    "            \"pos_similarity\": pos_similarities.mean().item(),\n",
    "            \"neg_similarity\": neg_similarities.mean().item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSearchEvaluator:\n",
    "    \"\"\"Evaluation metrics for embedding-based search\"\"\"\n",
    "    def __init__(self, k_values: List[int] = [1, 5, 10, 100]):\n",
    "        self.k_values = k_values\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(\n",
    "        self,\n",
    "        search_engine: EmbeddingSearchEngine,\n",
    "        eval_queries: List[Dict],\n",
    "        relevant_docs: Dict[int, List[int]],\n",
    "        batch_size: int = 32\n",
    "    ) -> Dict[str, float]:\n",
    "        metrics = defaultdict(list)\n",
    "        \n",
    "        for i in range(0, len(eval_queries), batch_size):\n",
    "            batch_queries = eval_queries[i:i + batch_size]\n",
    "            \n",
    "            for query in batch_queries:\n",
    "                query_id = query[\"id\"]\n",
    "                results = search_engine.search(\n",
    "                    query[\"text\"],\n",
    "                    k=max(self.k_values),\n",
    "                    return_scores=True\n",
    "                )\n",
    "                \n",
    "                # Get retrieved document IDs\n",
    "                retrieved_ids = [doc[\"id\"] for doc in results]\n",
    "                \n",
    "                # Compute metrics\n",
    "                relevant_ids = set(relevant_docs[query_id])\n",
    "                batch_metrics = self._compute_metrics(\n",
    "                    retrieved_ids,\n",
    "                    relevant_ids\n",
    "                )\n",
    "                \n",
    "                for metric, value in batch_metrics.items():\n",
    "                    metrics[metric].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        return {k: np.mean(v) for k, v in metrics.items()}\n",
    "    \n",
    "    def _compute_metrics(\n",
    "        self,\n",
    "        retrieved_ids: List[int],\n",
    "        relevant_ids: set\n",
    "    ) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Recall@k\n",
    "        for k in self.k_values:\n",
    "            retrieved_at_k = set(retrieved_ids[:k])\n",
    "            recall = len(retrieved_at_k & relevant_ids) / len(relevant_ids)\n",
    "            metrics[f\"recall@{k}\"] = recall\n",
    "        \n",
    "        # Precision@k\n",
    "        for k in self.k_values:\n",
    "            if k > len(retrieved_ids):\n",
    "                continue\n",
    "            retrieved_at_k = set(retrieved_ids[:k])\n",
    "            precision = len(retrieved_at_k & relevant_ids) / k\n",
    "            metrics[f\"precision@{k}\"] = precision\n",
    "        \n",
    "        # Mean Reciprocal Rank (MRR)\n",
    "        for i, doc_id in enumerate(retrieved_ids):\n",
    "            if doc_id in relevant_ids:\n",
    "                metrics[\"mrr\"] = 1.0 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            metrics[\"mrr\"] = 0.0\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTrainingDataGenerator:\n",
    "    \"\"\"Generate training data for embedding-based search\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 10000,\n",
    "        max_query_length: int = 10,\n",
    "        max_doc_length: int = 100,\n",
    "        num_docs: int = 100000\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_query_length = max_query_length\n",
    "        self.max_doc_length = max_doc_length\n",
    "        \n",
    "        # Generate synthetic document corpus\n",
    "        self.documents = self._generate_documents(num_docs)\n",
    "        \n",
    "        # Generate synthetic query-document relevance\n",
    "        self.relevance = self._generate_relevance()\n",
    "    \n",
    "    def _generate_documents(self, num_docs: int) -> List[Dict]:\n",
    "        \"\"\"Generate synthetic documents\"\"\"\n",
    "        documents = []\n",
    "        for i in range(num_docs):\n",
    "            length = np.random.randint(20, self.max_doc_length)\n",
    "            tokens = np.random.randint(0, self.vocab_size, size=length)\n",
    "            \n",
    "            documents.append({\n",
    "                \"id\": i,\n",
    "                \"tokens\": tokens,\n",
    "                \"length\": length,\n",
    "                \"embedding_quality\": np.random.random()  # Synthetic quality score\n",
    "            })\n",
    "        return documents\n",
    "    \n",
    "    def _generate_relevance(self) -> Dict[int, List[Dict]]:\n",
    "        \"\"\"Generate synthetic query-document relevance\"\"\"\n",
    "        relevance = {}\n",
    "        num_queries = len(self.documents) // 10  # 10 documents per query on average\n",
    "        \n",
    "        for i in range(num_queries):\n",
    "            # Generate synthetic query\n",
    "            length = np.random.randint(3, self.max_query_length)\n",
    "            tokens = np.random.randint(0, self.vocab_size, size=length)\n",
    "            \n",
    "            # Randomly select relevant documents\n",
    "            num_relevant = np.random.randint(1, 5)\n",
    "            relevant_docs = np.random.choice(\n",
    "                len(self.documents),\n",
    "                size=num_relevant,\n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            relevance[i] = {\n",
    "                \"query\": {\n",
    "                    \"id\": i,\n",
    "                    \"tokens\": tokens,\n",
    "                    \"length\": length\n",
    "                },\n",
    "                \"relevant_docs\": relevant_docs.tolist()\n",
    "            }\n",
    "        \n",
    "        return relevance\n",
    "    \n",
    "    def generate_batch(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        neg_ratio: int = 4\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate training batch with negative sampling\"\"\"\n",
    "        # Randomly select queries\n",
    "        query_ids = np.random.choice(list(self.relevance.keys()), batch_size)\n",
    "        \n",
    "        # Prepare tensors\n",
    "        query_tokens = []\n",
    "        query_lengths = []\n",
    "        pos_doc_tokens = []\n",
    "        pos_doc_lengths = []\n",
    "        neg_doc_tokens = []\n",
    "        neg_doc_lengths = []\n",
    "        \n",
    "        for qid in query_ids:\n",
    "            # Get query\n",
    "            query = self.relevance[qid][\"query\"]\n",
    "            query_tokens.append(query[\"tokens\"])\n",
    "            query_lengths.append(query[\"length\"])\n",
    "            \n",
    "            # Get positive document\n",
    "            pos_doc_id = np.random.choice(self.relevance[qid][\"relevant_docs\"])\n",
    "            pos_doc = self.documents[pos_doc_id]\n",
    "            pos_doc_tokens.append(pos_doc[\"tokens\"])\n",
    "            pos_doc_lengths.append(pos_doc[\"length\"])\n",
    "            \n",
    "            # Get negative document\n",
    "            for _ in range(neg_ratio):\n",
    "                while True:\n",
    "                    neg_doc_id = np.random.randint(0, len(self.documents))\n",
    "                    if neg_doc_id not in self.relevance[qid][\"relevant_docs\"]:\n",
    "                        break\n",
    "                \n",
    "                neg_doc = self.documents[neg_doc_id]\n",
    "                neg_doc_tokens.append(neg_doc[\"tokens\"])\n",
    "                neg_doc_lengths.append(neg_doc[\"length\"])\n",
    "        \n",
    "        # Convert to tensors with padding\n",
    "        return (\n",
    "            self._pad_sequences(query_tokens),\n",
    "            torch.tensor(query_lengths),\n",
    "            self._pad_sequences(pos_doc_tokens),\n",
    "            torch.tensor(pos_doc_lengths),\n",
    "            self._pad_sequences(neg_doc_tokens),\n",
    "            torch.tensor(neg_doc_lengths)\n",
    "        )\n",
    "    \n",
    "    def _pad_sequences(self, sequences: List[np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"Pad sequences to same length\"\"\"\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "        padded = np.zeros((len(sequences), max_len), dtype=np.int64)\n",
    "        \n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded[i, :len(seq)] = seq\n",
    "        \n",
    "        return torch.tensor(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_search():\n",
    "    # Initialize components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # Create model\n",
    "    encoder = DualEncoder(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    \n",
    "    # Create search engine\n",
    "    search_engine = EmbeddingSearchEngine(\n",
    "        encoder=encoder,\n",
    "        embedding_dim=embedding_dim,\n",
    "        index_type=\"IVFFlat\",\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = DualEncoderTrainer(encoder, device=device)\n",
    "    \n",
    "    # Create data generator\n",
    "    data_gen = EmbeddingTrainingDataGenerator(\n",
    "        vocab_size=vocab_size,\n",
    "        num_docs=100000\n",
    "    )\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = EmbeddingSearchEvaluator()\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        \n",
    "        # Training\n",
    "        encoder.train()\n",
    "        epoch_metrics = defaultdict(list)\n",
    "        \n",
    "        for batch in range(100):  # 100 batches per epoch\n",
    "            # Generate batch\n",
    "            (\n",
    "                query_tokens,\n",
    "                query_lengths,\n",
    "                pos_doc_tokens,\n",
    "                pos_doc_lengths,\n",
    "                neg_doc_tokens,\n",
    "                neg_doc_lengths\n",
    "            ) = data_gen.generate_batch(batch_size)\n",
    "            \n",
    "            # Train step\n",
    "            metrics = trainer.train_step(\n",
    "                query_tokens,\n",
    "                query_lengths,\n",
    "                pos_doc_tokens,\n",
    "                pos_doc_lengths,\n",
    "                neg_doc_tokens,\n",
    "                neg_doc_lengths\n",
    "            )\n",
    "            \n",
    "            for k, v in metrics.items():\n",
    "                epoch_metrics[k].append(v)\n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print(f\"Batch {batch}, Loss: {metrics['loss']:.4f}\")\n",
    "        \n",
    "        # Print epoch metrics\n",
    "        print(\"\\nTraining Metrics:\")\n",
    "        for k, v in epoch_metrics.items():\n",
    "            print(f\"{k}: {np.mean(v):.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        if epoch % 2 == 0:  # Evaluate every 2 epochs\n",
    "            print(\"\\nRebuilding search index...\")\n",
    "            search_engine.build_index(data_gen.documents)\n",
    "            \n",
    "            print(\"\\nEvaluating...\")\n",
    "            eval_metrics = evaluator.evaluate(\n",
    "                search_engine=search_engine,\n",
    "                eval_queries=[q[\"query\"] for q in data_gen.relevance.values()],\n",
    "                relevant_docs={\n",
    "                    q[\"query\"][\"id\"]: q[\"relevant_docs\"]\n",
    "                    for q in data_gen.relevance.values()\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(\"\\nEvaluation Metrics:\")\n",
    "            for metric, value in eval_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_search():\n",
    "    # Initialize components\n",
    "    encoder = DualEncoder(vocab_size=10000, embedding_dim=128)\n",
    "    \n",
    "    search_engine = EmbeddingSearchEngine(\n",
    "        encoder=encoder,\n",
    "        embedding_dim=128,\n",
    "        index_type=\"IVFFlat\"\n",
    "    )\n",
    "    \n",
    "    # Generate sample documents\n",
    "    data_gen = EmbeddingTrainingDataGenerator(num_docs=1000)\n",
    "    \n",
    "    # Build index\n",
    "    search_engine.build_index(data_gen.documents)\n",
    "    \n",
    "    # Example searches\n",
    "    queries = [\n",
    "        \"sample query 1\",\n",
    "        \"sample query 2\",\n",
    "        \"sample query 3\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        results = search_engine.search(query, k=5)\n",
    "        \n",
    "        print(\"\\nTop 5 Results:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. Document ID: {result['id']}, Score: {result['score']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_embedding_search()\n",
    "    example_search()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
