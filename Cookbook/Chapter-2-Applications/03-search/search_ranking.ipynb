{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Ranking System with TorchRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrec\n",
    "from typing import Dict, List, Tuple, Optional, NamedTuple\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torchrec.sparse.jagged_tensor import KeyedJaggedTensor\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search-Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchFeatures:\n",
    "    \"\"\"Search-specific features for ranking\"\"\"\n",
    "    # Query features\n",
    "    query_tokens: torch.Tensor          # Tokenized query\n",
    "    query_length: torch.Tensor          # Query length\n",
    "    query_embeddings: torch.Tensor      # Pre-computed query embeddings\n",
    "    \n",
    "    # Document features\n",
    "    doc_tokens: torch.Tensor            # Tokenized document\n",
    "    doc_length: torch.Tensor            # Document length\n",
    "    doc_embeddings: torch.Tensor        # Pre-computed document embeddings\n",
    "    doc_static_features: torch.Tensor   # PageRank, freshness, etc.\n",
    "    \n",
    "    # Interaction features\n",
    "    exact_match_signals: torch.Tensor   # BM25, TF-IDF, etc.\n",
    "    semantic_match_signals: torch.Tensor # Cosine similarity, etc.\n",
    "    \n",
    "    # User context\n",
    "    user_id: torch.Tensor\n",
    "    search_history: torch.Tensor\n",
    "    session_features: torch.Tensor\n",
    "\n",
    "    def to(self, device: torch.device) -> 'SearchFeatures':\n",
    "        return SearchFeatures(\n",
    "            **{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "               for k, v in self.__dict__.items()}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Encoder Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(torch.nn.Module):\n",
    "    \"\"\"Transformer-based encoder for query/document\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.pooling = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, embedding_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        embeddings: torch.Tensor,\n",
    "        lengths: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Create attention mask based on lengths\n",
    "        max_len = embeddings.size(1)\n",
    "        mask = torch.arange(max_len, device=embeddings.device)[None, :] >= lengths[:, None]\n",
    "        \n",
    "        # Encode sequence\n",
    "        encoded = self.transformer(embeddings, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Pool sequence (using attention mask)\n",
    "        masked_encoded = encoded.masked_fill(mask.unsqueeze(-1), 0)\n",
    "        pooled = masked_encoded.sum(dim=1) / lengths.unsqueeze(-1)\n",
    "        \n",
    "        return self.pooling(pooled)\n",
    "\n",
    "class CrossAttention(torch.nn.Module):\n",
    "    \"\"\"Cross-attention between query and document\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = torch.nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        document: torch.Tensor,\n",
    "        query_mask: Optional[torch.Tensor] = None,\n",
    "        doc_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Query-to-document attention\n",
    "        q2d_attended, q2d_weights = self.attention(\n",
    "            query, document, document,\n",
    "            key_padding_mask=doc_mask\n",
    "        )\n",
    "        q2d_attended = self.norm(query + self.dropout(q2d_attended))\n",
    "        \n",
    "        # Document-to-query attention\n",
    "        d2q_attended, d2q_weights = self.attention(\n",
    "            document, query, query,\n",
    "            key_padding_mask=query_mask\n",
    "        )\n",
    "        d2q_attended = self.norm(document + self.dropout(d2q_attended))\n",
    "        \n",
    "        return q2d_attended, d2q_attended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchRankingModel(torch.nn.Module):\n",
    "    \"\"\"Neural ranking model for search\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_dim: int = 256,\n",
    "        num_transformer_layers: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Query encoder\n",
    "        self.query_encoder = TransformerEncoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_layers=num_transformer_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Document encoder\n",
    "        self.doc_encoder = TransformerEncoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_layers=num_transformer_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Cross attention\n",
    "        self.cross_attention = CrossAttention(\n",
    "            embedding_dim=embedding_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # User embedding\n",
    "        self.user_embedding = torchrec.EmbeddingBagCollection(\n",
    "            tables=[\n",
    "                torchrec.EmbeddingBagConfig(\n",
    "                    name=\"user_embeddings\",\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=1_000_000,  # Adjust based on your needs\n",
    "                    feature_names=[\"user_id\"]\n",
    "                )\n",
    "            ],\n",
    "            device=torch.device(\"meta\")\n",
    "        )\n",
    "        \n",
    "        # Static feature processing\n",
    "        self.static_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Session feature processing\n",
    "        self.session_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Final scoring layers\n",
    "        self.scoring_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim * 6, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: SearchFeatures) -> Dict[str, torch.Tensor]:\n",
    "        # Encode query and document\n",
    "        query_encoding = self.query_encoder(\n",
    "            features.query_embeddings,\n",
    "            features.query_length\n",
    "        )\n",
    "        \n",
    "        doc_encoding = self.doc_encoder(\n",
    "            features.doc_embeddings,\n",
    "            features.doc_length\n",
    "        )\n",
    "        \n",
    "        # Cross attention\n",
    "        query_attended, doc_attended = self.cross_attention(\n",
    "            features.query_embeddings,\n",
    "            features.doc_embeddings\n",
    "        )\n",
    "        \n",
    "        # Get user embedding\n",
    "        user_embedding = self.user_embedding(\n",
    "            KeyedJaggedTensor.from_lengths_sync(\n",
    "                keys=[\"user_id\"],\n",
    "                values=features.user_id,\n",
    "                lengths=torch.ones_like(features.user_id)\n",
    "            )\n",
    "        ).values()\n",
    "        \n",
    "        # Process static and session features\n",
    "        static_features = self.static_encoder(features.doc_static_features)\n",
    "        session_features = self.session_encoder(features.session_features)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([\n",
    "            query_encoding,\n",
    "            doc_encoding,\n",
    "            query_attended.mean(dim=1),\n",
    "            doc_attended.mean(dim=1),\n",
    "            static_features,\n",
    "            session_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Generate ranking score\n",
    "        ranking_score = self.scoring_layers(combined_features)\n",
    "        \n",
    "        return {\n",
    "            \"score\": ranking_score,\n",
    "            \"query_encoding\": query_encoding,\n",
    "            \"doc_encoding\": doc_encoding,\n",
    "            \"attention_weights\": {\n",
    "                \"query_to_doc\": query_attended,\n",
    "                \"doc_to_query\": doc_attended\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search-Specific Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchRankingLoss:\n",
    "    \"\"\"Multiple loss components for search ranking\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        margin: float = 0.1,\n",
    "        lambda_semantic: float = 0.1,\n",
    "        lambda_relevance: float = 1.0,\n",
    "        lambda_diversity: float = 0.1\n",
    "    ):\n",
    "        self.margin = margin\n",
    "        self.lambda_semantic = lambda_semantic\n",
    "        self.lambda_relevance = lambda_relevance\n",
    "        self.lambda_diversity = lambda_diversity\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        outputs: Dict[str, torch.Tensor],\n",
    "        labels: torch.Tensor,\n",
    "        features: SearchFeatures\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        scores = outputs[\"score\"]\n",
    "        query_encoding = outputs[\"query_encoding\"]\n",
    "        doc_encoding = outputs[\"doc_encoding\"]\n",
    "        \n",
    "        # Relevance loss (pairwise ranking)\n",
    "        relevance_loss = self._compute_relevance_loss(scores, labels)\n",
    "        \n",
    "        # Semantic matching loss\n",
    "        semantic_loss = self._compute_semantic_loss(\n",
    "            query_encoding,\n",
    "            doc_encoding,\n",
    "            features.semantic_match_signals\n",
    "        )\n",
    "        \n",
    "        # Diversity loss\n",
    "        diversity_loss = self._compute_diversity_loss(doc_encoding)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            self.lambda_relevance * relevance_loss +\n",
    "            self.lambda_semantic * semantic_loss +\n",
    "            self.lambda_diversity * diversity_loss\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"relevance_loss\": relevance_loss,\n",
    "            \"semantic_loss\": semantic_loss,\n",
    "            \"diversity_loss\": diversity_loss\n",
    "        }\n",
    "    \n",
    "    def _compute_relevance_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            scores.squeeze(),\n",
    "            labels.float()\n",
    "        )\n",
    "    \n",
    "    def _compute_semantic_loss(\n",
    "        self,\n",
    "        query_encoding: torch.Tensor,\n",
    "        doc_encoding: torch.Tensor,\n",
    "        semantic_signals: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Cosine similarity between predicted and pre-computed semantic signals\n",
    "        pred_similarity = torch.nn.functional.cosine_similarity(\n",
    "            query_encoding,\n",
    "            doc_encoding\n",
    "        )\n",
    "        return torch.nn.functional.mse_loss(\n",
    "            pred_similarity,\n",
    "            semantic_signals\n",
    "        )\n",
    "    \n",
    "    def _compute_diversity_loss(\n",
    "        self,\n",
    "        doc_encoding: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Encourage diversity in document representations\n",
    "        similarity_matrix = torch.mm(\n",
    "            doc_encoding,\n",
    "            doc_encoding.t()\n",
    "        )\n",
    "        \n",
    "        # Remove self-similarity from diagonal\n",
    "        mask = torch.eye(\n",
    "            similarity_matrix.size(0),\n",
    "            device=similarity_matrix.device\n",
    "        )\n",
    "        similarity_matrix = similarity_matrix * (1 - mask)\n",
    "        \n",
    "        # Minimize pairwise similarities\n",
    "        return similarity_matrix.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchDataGenerator:\n",
    "    \"\"\"Generate synthetic search data\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 50000,\n",
    "        num_users: int = 100000,\n",
    "        num_docs: int = 1000000,\n",
    "        embedding_dim: int = 128,\n",
    "        max_query_length: int = 10,\n",
    "        max_doc_length: int = 100\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_users = num_users\n",
    "        self.num_docs = num_docs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_query_length = max_query_length\n",
    "        self.max_doc_length = max_doc_length\n",
    "        \n",
    "        # Generate synthetic word embeddings\n",
    "        self.word_embeddings = torch.randn(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Generate synthetic document features\n",
    "        self.doc_features = {\n",
    "            \"static\": torch.randn(num_docs, embedding_dim),\n",
    "            \"pagerank\": torch.rand(num_docs),\n",
    "            \"freshness\": torch.rand(num_docs),\n",
    "            \"quality_score\": torch.rand(num_docs)\n",
    "        }\n",
    "        \n",
    "        # Generate user features\n",
    "        self.user_features = {\n",
    "            \"session_embeddings\": torch.randn(num_users, embedding_dim),\n",
    "            \"search_history\": self._generate_search_history()\n",
    "        }\n",
    "    \n",
    "    def _generate_search_history(self) -> Dict[int, List[int]]:\n",
    "        \"\"\"Generate synthetic search history for users\"\"\"\n",
    "        history = {}\n",
    "        for user_id in range(self.num_users):\n",
    "            num_searches = np.random.randint(5, 20)\n",
    "            history[user_id] = np.random.choice(\n",
    "                self.num_docs,\n",
    "                size=num_searches,\n",
    "                replace=False\n",
    "            ).tolist()\n",
    "        return history\n",
    "    \n",
    "    def _generate_query(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate synthetic query\"\"\"\n",
    "        length = torch.randint(2, self.max_query_length + 1, (1,)).item()\n",
    "        tokens = torch.randint(0, self.vocab_size, (length,))\n",
    "        embeddings = self.word_embeddings[tokens]\n",
    "        return tokens, torch.tensor(length), embeddings\n",
    "    \n",
    "    def _generate_document(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate synthetic document\"\"\"\n",
    "        length = torch.randint(10, self.max_doc_length + 1, (1,)).item()\n",
    "        tokens = torch.randint(0, self.vocab_size, (length,))\n",
    "        embeddings = self.word_embeddings[tokens]\n",
    "        return tokens, torch.tensor(length), embeddings\n",
    "    \n",
    "    def _compute_relevance(\n",
    "        self,\n",
    "        query_emb: torch.Tensor,\n",
    "        doc_emb: torch.Tensor,\n",
    "        doc_features: Dict[str, torch.Tensor]\n",
    "    ) -> float:\n",
    "        \"\"\"Compute synthetic relevance score\"\"\"\n",
    "        semantic_sim = torch.nn.functional.cosine_similarity(\n",
    "            query_emb.mean(0, keepdim=True),\n",
    "            doc_emb.mean(0, keepdim=True)\n",
    "        ).item()\n",
    "        \n",
    "        quality = doc_features[\"quality_score\"].item()\n",
    "        freshness = doc_features[\"freshness\"].item()\n",
    "        \n",
    "        return (semantic_sim * 0.5 + quality * 0.3 + freshness * 0.2)\n",
    "    \n",
    "    def generate_batch(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        pos_ratio: float = 0.2\n",
    "    ) -> Tuple[SearchFeatures, torch.Tensor]:\n",
    "        # Generate queries and documents\n",
    "        queries = [self._generate_query() for _ in range(batch_size)]\n",
    "        docs = [self._generate_document() for _ in range(batch_size)]\n",
    "        \n",
    "        # Generate user IDs and features\n",
    "        user_ids = torch.randint(0, self.num_users, (batch_size,))\n",
    "        session_features = self.user_features[\"session_embeddings\"][user_ids]\n",
    "        \n",
    "        # Compute match signals\n",
    "        exact_match = torch.tensor([\n",
    "            len(set(q[0].tolist()) & set(d[0].tolist())) / min(len(q[0]), len(d[0]))\n",
    "            for q, d in zip(queries, docs)\n",
    "        ])\n",
    "        \n",
    "        semantic_match = torch.tensor([\n",
    "            torch.nn.functional.cosine_similarity(\n",
    "                q[2].mean(0, keepdim=True),\n",
    "                d[2].mean(0, keepdim=True)\n",
    "            ).item()\n",
    "            for q, d in zip(queries, docs)\n",
    "        ])\n",
    "        \n",
    "        # Compute relevance scores and generate labels\n",
    "        relevance_scores = torch.tensor([\n",
    "            self._compute_relevance(q[2], d[2], {\n",
    "                k: v[i] for k, v in self.doc_features.items()\n",
    "            })\n",
    "            for i, (q, d) in enumerate(zip(queries, docs))\n",
    "        ])\n",
    "        \n",
    "        labels = torch.bernoulli(relevance_scores * pos_ratio)\n",
    "        \n",
    "        # Create features object\n",
    "        features = SearchFeatures(\n",
    "            query_tokens=torch.nn.utils.rnn.pad_sequence(\n",
    "                [q[0] for q in queries],\n",
    "                batch_first=True\n",
    "            ),\n",
    "            query_length=torch.stack([q[1] for q in queries]),\n",
    "            query_embeddings=torch.nn.utils.rnn.pad_sequence(\n",
    "                [q[2] for q in queries],\n",
    "                batch_first=True\n",
    "            ),\n",
    "            doc_tokens=torch.nn.utils.rnn.pad_sequence(\n",
    "                [d[0] for d in docs],\n",
    "                batch_first=True\n",
    "            ),\n",
    "            doc_length=torch.stack([d[1] for d in docs]),\n",
    "            doc_embeddings=torch.nn.utils.rnn.pad_sequence(\n",
    "                [d[2] for d in docs],\n",
    "                batch_first=True\n",
    "            ),\n",
    "            doc_static_features=torch.stack([\n",
    "                self.doc_features[\"static\"][i] for i in range(batch_size)\n",
    "            ]),\n",
    "            exact_match_signals=exact_match,\n",
    "            semantic_match_signals=semantic_match,\n",
    "            user_id=user_ids,\n",
    "            search_history=torch.tensor([\n",
    "                self.user_features[\"search_history\"][uid.item()][:5]\n",
    "                for uid in user_ids\n",
    "            ]),\n",
    "            session_features=session_features\n",
    "        )\n",
    "        \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTrainer:\n",
    "    \"\"\"Training infrastructure for search ranking\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SearchRankingModel,\n",
    "        loss_fn: SearchRankingLoss,\n",
    "        learning_rate: float = 0.001,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler with warmup\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=learning_rate,\n",
    "            total_steps=1000,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        features: SearchFeatures,\n",
    "        labels: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Move to device\n",
    "        features = features.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(features)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses = self.loss_fn.compute_loss(outputs, labels, features)\n",
    "        \n",
    "        # Backward pass\n",
    "        losses[\"total_loss\"].backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return {k: v.item() for k, v in losses.items()}\n",
    "\n",
    "# [MARKDOWN: 7. Search Evaluation Metrics]\n",
    "\n",
    "class SearchEvaluator:\n",
    "    \"\"\"Evaluation metrics for search ranking\"\"\"\n",
    "    def __init__(self, k_values: List[int] = [1, 3, 5, 10]):\n",
    "        self.k_values = k_values\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(\n",
    "        self,\n",
    "        model: SearchRankingModel,\n",
    "        features: SearchFeatures,\n",
    "        labels: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        model.eval()\n",
    "        outputs = model(features)\n",
    "        scores = outputs[\"score\"].squeeze()\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Ranking metrics\n",
    "        metrics.update(self._compute_ranking_metrics(scores, labels))\n",
    "        \n",
    "        # Diversity metrics\n",
    "        metrics.update(self._compute_diversity_metrics(\n",
    "            outputs[\"doc_encoding\"],\n",
    "            scores,\n",
    "            labels\n",
    "        ))\n",
    "        \n",
    "        # Semantic matching metrics\n",
    "        metrics.update(self._compute_semantic_metrics(\n",
    "            outputs[\"query_encoding\"],\n",
    "            outputs[\"doc_encoding\"],\n",
    "            features.semantic_match_signals\n",
    "        ))\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_ranking_metrics(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Sort by scores\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        # NDCG@k\n",
    "        for k in self.k_values:\n",
    "            metrics[f\"ndcg@{k}\"] = self._compute_ndcg(sorted_labels, k)\n",
    "        \n",
    "        # Precision@k\n",
    "        for k in self.k_values:\n",
    "            metrics[f\"precision@{k}\"] = sorted_labels[:k].float().mean().item()\n",
    "        \n",
    "        # Mean Reciprocal Rank\n",
    "        pos_indices = (sorted_labels == 1).nonzero()\n",
    "        if len(pos_indices) > 0:\n",
    "            metrics[\"mrr\"] = 1.0 / (pos_indices[0].item() + 1)\n",
    "        else:\n",
    "            metrics[\"mrr\"] = 0.0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_diversity_metrics(\n",
    "        self,\n",
    "        doc_encodings: torch.Tensor,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Get top-k document encodings\n",
    "        top_k = 10\n",
    "        top_indices = torch.argsort(scores, descending=True)[:top_k]\n",
    "        top_encodings = doc_encodings[top_indices]\n",
    "        \n",
    "        # Compute pairwise similarities\n",
    "        similarity_matrix = torch.mm(top_encodings, top_encodings.t())\n",
    "        \n",
    "        # Average similarity (lower is more diverse)\n",
    "        metrics[\"diversity\"] = similarity_matrix.mean().item()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_semantic_metrics(\n",
    "        self,\n",
    "        query_encoding: torch.Tensor,\n",
    "        doc_encoding: torch.Tensor,\n",
    "        semantic_signals: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Compute predicted similarities\n",
    "        pred_similarity = torch.nn.functional.cosine_similarity(\n",
    "            query_encoding,\n",
    "            doc_encoding\n",
    "        )\n",
    "        \n",
    "        # Correlation with pre-computed semantic signals\n",
    "        correlation = torch.corrcoef(\n",
    "            torch.stack([pred_similarity, semantic_signals])\n",
    "        )[0, 1].item()\n",
    "        \n",
    "        metrics[\"semantic_correlation\"] = correlation\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_ndcg(labels: torch.Tensor, k: int) -> float:\n",
    "        if k > len(labels):\n",
    "            k = len(labels)\n",
    "        \n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "        \n",
    "        # Calculate DCG\n",
    "        for i in range(k):\n",
    "            if labels[i] == 1:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        \n",
    "        # Calculate IDCG\n",
    "        sorted_labels = torch.sort(labels, descending=True)[0]\n",
    "        for i in range(k):\n",
    "            if sorted_labels[i] == 1:\n",
    "                idcg += 1 / np.log2(i + 2)\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_search_model():\n",
    "    # Initialize components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SearchRankingModel(\n",
    "        embedding_dim=128,\n",
    "        hidden_dim=256\n",
    "    )\n",
    "    \n",
    "    loss_fn = SearchRankingLoss()\n",
    "    trainer = SearchTrainer(model, loss_fn, device=device)\n",
    "    evaluator = SearchEvaluator()\n",
    "    \n",
    "    data_gen = SearchDataGenerator(\n",
    "        vocab_size=50000,\n",
    "        num_users=100000,\n",
    "        num_docs=1000000,\n",
    "        embedding_dim=128\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    batch_size = 64\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_losses = defaultdict(list)\n",
    "        \n",
    "        for batch in range(100):  # 100 batches per epoch\n",
    "            features, labels = data_gen.generate_batch(batch_size)\n",
    "            losses = trainer.train_step(features, labels)\n",
    "            \n",
    "            for k, v in losses.items():\n",
    "                epoch_losses[k].append(v)\n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print(f\"Batch {batch}, Loss: {losses['total_loss']:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\nEvaluation:\")\n",
    "        eval_features, eval_labels = data_gen.generate_batch(1000)\n",
    "        eval_metrics = evaluator.evaluate(\n",
    "            model,\n",
    "            eval_features.to(device),\n",
    "            eval_labels.to(device)\n",
    "        )\n",
    "        \n",
    "        for metric, value in eval_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_search_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
