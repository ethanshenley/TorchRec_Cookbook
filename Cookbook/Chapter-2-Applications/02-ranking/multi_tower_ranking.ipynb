{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Tower Ranking System with TorchRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrec\n",
    "from typing import Dict, List, Tuple, NamedTuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torchrec.sparse.jagged_tensor import KeyedJaggedTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Tower Feature Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TowerFeatures:\n",
    "    \"\"\"Features for each tower\"\"\"\n",
    "    dense_features: torch.Tensor\n",
    "    sparse_ids: torch.Tensor\n",
    "    sparse_weights: Optional[torch.Tensor] = None\n",
    "\n",
    "class MultiTowerFeatures:\n",
    "    \"\"\"Complete feature set for multi-tower model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_tower: TowerFeatures,\n",
    "        item_tower: TowerFeatures,\n",
    "        context_tower: TowerFeatures,\n",
    "        sequence_tower: Optional[TowerFeatures] = None\n",
    "    ):\n",
    "        self.user_tower = user_tower\n",
    "        self.item_tower = item_tower\n",
    "        self.context_tower = context_tower\n",
    "        self.sequence_tower = sequence_tower\n",
    "    \n",
    "    def to(self, device: torch.device) -> 'MultiTowerFeatures':\n",
    "        return MultiTowerFeatures(\n",
    "            user_tower=TowerFeatures(\n",
    "                dense_features=self.user_tower.dense_features.to(device),\n",
    "                sparse_ids=self.user_tower.sparse_ids.to(device),\n",
    "                sparse_weights=self.user_tower.sparse_weights.to(device) \n",
    "                    if self.user_tower.sparse_weights is not None else None\n",
    "            ),\n",
    "            item_tower=TowerFeatures(\n",
    "                dense_features=self.item_tower.dense_features.to(device),\n",
    "                sparse_ids=self.item_tower.sparse_ids.to(device),\n",
    "                sparse_weights=self.item_tower.sparse_weights.to(device)\n",
    "                    if self.item_tower.sparse_weights is not None else None\n",
    "            ),\n",
    "            context_tower=TowerFeatures(\n",
    "                dense_features=self.context_tower.dense_features.to(device),\n",
    "                sparse_ids=self.context_tower.sparse_ids.to(device),\n",
    "                sparse_weights=self.context_tower.sparse_weights.to(device)\n",
    "                    if self.context_tower.sparse_weights is not None else None\n",
    "            ),\n",
    "            sequence_tower=TowerFeatures(\n",
    "                dense_features=self.sequence_tower.dense_features.to(device),\n",
    "                sparse_ids=self.sequence_tower.sparse_ids.to(device),\n",
    "                sparse_weights=self.sequence_tower.sparse_weights.to(device)\n",
    "                    if self.sequence_tower is not None and \n",
    "                       self.sequence_tower.sparse_weights is not None else None\n",
    "            ) if self.sequence_tower is not None else None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tower Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tower(torch.nn.Module):\n",
    "    \"\"\"Single tower of multi-tower architecture\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_tables: torchrec.EmbeddingBagCollection,\n",
    "        dense_dim: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dims: List[int],\n",
    "        dropout: float = 0.1,\n",
    "        use_batch_norm: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_tables = embedding_tables\n",
    "        input_dim = dense_dim + embedding_dim\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                torch.nn.Linear(prev_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout)\n",
    "            ])\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                layers.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, features: TowerFeatures) -> torch.Tensor:\n",
    "        # Get embeddings\n",
    "        sparse_embeddings = self.embedding_tables(\n",
    "            KeyedJaggedTensor.from_lengths_sync(\n",
    "                keys=[\"sparse_id\"],\n",
    "                values=features.sparse_ids,\n",
    "                lengths=torch.ones(len(features.sparse_ids))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Combine dense and sparse features\n",
    "        combined_features = torch.cat([\n",
    "            features.dense_features,\n",
    "            sparse_embeddings.values()\n",
    "        ], dim=1)\n",
    "        \n",
    "        return self.network(combined_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Tower Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossInteraction(torch.nn.Module):\n",
    "    \"\"\"Cross interaction between tower outputs\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_layers: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, input_dim),\n",
    "                torch.nn.LayerNorm(input_dim),\n",
    "                torch.nn.ReLU()\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.attention = torch.nn.MultiheadAttention(\n",
    "            embed_dim=input_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, tower_outputs: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # Stack tower outputs\n",
    "        stacked = torch.stack(tower_outputs, dim=1)  # [batch_size, num_towers, dim]\n",
    "        \n",
    "        # Self-attention across towers\n",
    "        attended, _ = self.attention(stacked, stacked, stacked)\n",
    "        \n",
    "        # Cross layers\n",
    "        cross_features = attended\n",
    "        for layer in self.layers:\n",
    "            cross_features = layer(cross_features) + cross_features\n",
    "        \n",
    "        return cross_features.mean(dim=1)  # Pool across towers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTowerModel(torch.nn.Module):\n",
    "    \"\"\"Complete multi-tower ranking model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tower_configs: Dict[str, Dict],\n",
    "        final_hidden_dims: List[int],\n",
    "        embedding_dim: int = 64,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize towers\n",
    "        self.towers = torch.nn.ModuleDict()\n",
    "        for tower_name, config in tower_configs.items():\n",
    "            embedding_tables = torchrec.EmbeddingBagCollection(\n",
    "                tables=[\n",
    "                    torchrec.EmbeddingBagConfig(\n",
    "                        name=f\"{tower_name}_embeddings\",\n",
    "                        embedding_dim=embedding_dim,\n",
    "                        num_embeddings=config[\"num_embeddings\"],\n",
    "                        feature_names=[\"sparse_id\"]\n",
    "                    )\n",
    "                ],\n",
    "                device=torch.device(\"meta\")\n",
    "            )\n",
    "            \n",
    "            self.towers[tower_name] = Tower(\n",
    "                embedding_tables=embedding_tables,\n",
    "                dense_dim=config[\"dense_dim\"],\n",
    "                embedding_dim=embedding_dim,\n",
    "                hidden_dims=config[\"hidden_dims\"],\n",
    "                dropout=dropout\n",
    "            )\n",
    "        \n",
    "        # Cross interaction\n",
    "        self.cross_interaction = CrossInteraction(\n",
    "            input_dim=config[\"hidden_dims\"][-1]\n",
    "        )\n",
    "        \n",
    "        # Final layers\n",
    "        layers = []\n",
    "        prev_dim = config[\"hidden_dims\"][-1]\n",
    "        \n",
    "        for hidden_dim in final_hidden_dims:\n",
    "            layers.extend([\n",
    "                torch.nn.Linear(prev_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout),\n",
    "                torch.nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(torch.nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.final_layers = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        features: MultiTowerFeatures\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Process each tower\n",
    "        tower_outputs = []\n",
    "        \n",
    "        for tower_name, tower in self.towers.items():\n",
    "            tower_features = getattr(features, f\"{tower_name}_tower\")\n",
    "            tower_output = tower(tower_features)\n",
    "            tower_outputs.append(tower_output)\n",
    "        \n",
    "        # Cross interaction\n",
    "        cross_features = self.cross_interaction(tower_outputs)\n",
    "        \n",
    "        # Final prediction\n",
    "        ranking_score = self.final_layers(cross_features)\n",
    "        \n",
    "        return {\n",
    "            \"score\": ranking_score,\n",
    "            \"tower_embeddings\": {\n",
    "                name: output for name, output in zip(self.towers.keys(), tower_outputs)\n",
    "            },\n",
    "            \"cross_features\": cross_features\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTowerLoss:\n",
    "    \"\"\"Multi-objective loss for multi-tower model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        margin: float = 0.1,\n",
    "        tower_weights: Optional[Dict[str, float]] = None\n",
    "    ):\n",
    "        self.margin = margin\n",
    "        self.tower_weights = tower_weights or {\n",
    "            \"user\": 1.0,\n",
    "            \"item\": 1.0,\n",
    "            \"context\": 0.5\n",
    "        }\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        outputs: Dict[str, torch.Tensor],\n",
    "        labels: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        scores = outputs[\"score\"]\n",
    "        tower_embeddings = outputs[\"tower_embeddings\"]\n",
    "        cross_features = outputs[\"cross_features\"]\n",
    "        \n",
    "        # Main ranking loss\n",
    "        ranking_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            scores.squeeze(),\n",
    "            labels.float()\n",
    "        )\n",
    "        \n",
    "        # Tower regularization\n",
    "        tower_losses = {}\n",
    "        for tower_name, embeddings in tower_embeddings.items():\n",
    "            tower_losses[f\"{tower_name}_reg\"] = (\n",
    "                self.tower_weights[tower_name] * \n",
    "                torch.norm(embeddings, p=2, dim=1).mean()\n",
    "            )\n",
    "        \n",
    "        # Contrastive loss\n",
    "        pos_features = cross_features[labels == 1]\n",
    "        neg_features = cross_features[labels == 0]\n",
    "        \n",
    "        if len(pos_features) > 0 and len(neg_features) > 0:\n",
    "            contrastive_loss = torch.nn.functional.triplet_margin_loss(\n",
    "                anchor=pos_features,\n",
    "                positive=pos_features.roll(1, dims=0),  # Positive pairs\n",
    "                negative=neg_features,\n",
    "                margin=self.margin\n",
    "            )\n",
    "        else:\n",
    "            contrastive_loss = torch.tensor(0.0, device=scores.device)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = ranking_loss + contrastive_loss\n",
    "        for tower_loss in tower_losses.values():\n",
    "            total_loss += tower_loss\n",
    "        \n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"ranking_loss\": ranking_loss,\n",
    "            \"contrastive_loss\": contrastive_loss,\n",
    "            **tower_losses\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Tower Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTowerDataGenerator:\n",
    "    \"\"\"Generate synthetic data for multi-tower model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        num_contexts: int,\n",
    "        user_dense_dim: int = 10,\n",
    "        item_dense_dim: int = 20,\n",
    "        context_dense_dim: int = 5,\n",
    "        sequence_length: int = 10\n",
    "    ):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_contexts = num_contexts\n",
    "        \n",
    "        # Generate synthetic user features\n",
    "        self.user_features = {\n",
    "            \"dense\": torch.randn(num_users, user_dense_dim),\n",
    "            \"categories\": torch.randint(0, 10, (num_users,)),\n",
    "            \"activity_level\": torch.rand(num_users)\n",
    "        }\n",
    "        \n",
    "        # Generate synthetic item features\n",
    "        self.item_features = {\n",
    "            \"dense\": torch.randn(num_items, item_dense_dim),\n",
    "            \"categories\": torch.randint(0, 20, (num_items,)),\n",
    "            \"popularity\": torch.rand(num_items)\n",
    "        }\n",
    "        \n",
    "        # Generate synthetic context features\n",
    "        self.context_features = {\n",
    "            \"dense\": torch.randn(num_contexts, context_dense_dim),\n",
    "            \"time_bins\": torch.randint(0, 24, (num_contexts,)),\n",
    "            \"device_types\": torch.randint(0, 5, (num_contexts,))\n",
    "        }\n",
    "        \n",
    "        # Generate interaction patterns\n",
    "        self.interaction_patterns = self._generate_interaction_patterns()\n",
    "    \n",
    "    def _generate_interaction_patterns(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Generate interaction patterns between different features\"\"\"\n",
    "        return {\n",
    "            \"user_item\": torch.rand(self.num_users, self.num_items),\n",
    "            \"user_context\": torch.rand(self.num_users, self.num_contexts),\n",
    "            \"item_context\": torch.rand(self.num_items, self.num_contexts)\n",
    "        }\n",
    "    \n",
    "    def generate_batch(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        include_sequence: bool = True\n",
    "    ) -> Tuple[MultiTowerFeatures, torch.Tensor]:\n",
    "        # Sample IDs\n",
    "        user_ids = torch.randint(0, self.num_users, (batch_size,))\n",
    "        item_ids = torch.randint(0, self.num_items, (batch_size,))\n",
    "        context_ids = torch.randint(0, self.num_contexts, (batch_size,))\n",
    "        \n",
    "        # Generate tower features\n",
    "        user_tower = TowerFeatures(\n",
    "            dense_features=self.user_features[\"dense\"][user_ids],\n",
    "            sparse_ids=user_ids,\n",
    "            sparse_weights=self.user_features[\"activity_level\"][user_ids]\n",
    "        )\n",
    "        \n",
    "        item_tower = TowerFeatures(\n",
    "            dense_features=self.item_features[\"dense\"][item_ids],\n",
    "            sparse_ids=item_ids,\n",
    "            sparse_weights=self.item_features[\"popularity\"][item_ids]\n",
    "        )\n",
    "        \n",
    "        context_tower = TowerFeatures(\n",
    "            dense_features=self.context_features[\"dense\"][context_ids],\n",
    "            sparse_ids=context_ids\n",
    "        )\n",
    "        \n",
    "        # Generate sequence features if needed\n",
    "        sequence_tower = None\n",
    "        if include_sequence:\n",
    "            seq_length = 10\n",
    "            history_ids = torch.randint(\n",
    "                0, self.num_items,\n",
    "                (batch_size, seq_length)\n",
    "            )\n",
    "            \n",
    "            sequence_tower = TowerFeatures(\n",
    "                dense_features=torch.stack([\n",
    "                    self.item_features[\"dense\"][history_ids[i]].mean(0)\n",
    "                    for i in range(batch_size)\n",
    "                ]),\n",
    "                sparse_ids=history_ids.view(-1)\n",
    "            )\n",
    "        \n",
    "        features = MultiTowerFeatures(\n",
    "            user_tower=user_tower,\n",
    "            item_tower=item_tower,\n",
    "            context_tower=context_tower,\n",
    "            sequence_tower=sequence_tower\n",
    "        )\n",
    "        \n",
    "        # Generate labels based on interaction patterns\n",
    "        interaction_scores = (\n",
    "            self.interaction_patterns[\"user_item\"][user_ids, item_ids] * 0.4 +\n",
    "            self.interaction_patterns[\"user_context\"][user_ids, context_ids] * 0.3 +\n",
    "            self.interaction_patterns[\"item_context\"][item_ids, context_ids] * 0.3\n",
    "        )\n",
    "        \n",
    "        labels = torch.bernoulli(interaction_scores * 0.2)  # 20% positive rate\n",
    "        \n",
    "        return features, labels"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
