{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ranking System with TorchRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrec\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, NamedTuple, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from torchrec.sparse.jagged_tensor import KeyedJaggedTensor\n",
    "from utils.data_generators import TorchRecDataGenerator\n",
    "from utils.debugging import TorchRecDebugger\n",
    "from utils.benchmark import TorchRecBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RankingFeatures:\n",
    "    \"\"\"Complex feature set for ranking\"\"\"\n",
    "    # Dense features\n",
    "    user_features: torch.Tensor      # age, gender, etc.\n",
    "    item_features: torch.Tensor      # price, age, etc.\n",
    "    context_features: torch.Tensor   # time, device, etc.\n",
    "    \n",
    "    # Sparse features (IDs)\n",
    "    user_id: torch.Tensor\n",
    "    item_id: torch.Tensor\n",
    "    category_id: torch.Tensor\n",
    "    \n",
    "    # Interaction features\n",
    "    historical_ctr: torch.Tensor    # Historical click-through rate\n",
    "    user_item_similarity: torch.Tensor\n",
    "    \n",
    "    # Sequence features\n",
    "    history_length: torch.Tensor\n",
    "    position_ids: torch.Tensor\n",
    "\n",
    "class FeatureProcessor:\n",
    "    \"\"\"Process and normalize features\"\"\"\n",
    "    def __init__(self, feature_config: Dict[str, Dict]):\n",
    "        self.feature_config = feature_config\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def fit(self, features: Dict[str, torch.Tensor]):\n",
    "        \"\"\"Compute normalization parameters\"\"\"\n",
    "        for name, config in self.feature_config.items():\n",
    "            if config[\"type\"] == \"continuous\":\n",
    "                self.scalers[name] = {\n",
    "                    \"mean\": features[name].mean(),\n",
    "                    \"std\": features[name].std()\n",
    "                }\n",
    "    \n",
    "    def transform(self, features: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Normalize features\"\"\"\n",
    "        normalized = {}\n",
    "        for name, tensor in features.items():\n",
    "            config = self.feature_config[name]\n",
    "            \n",
    "            if config[\"type\"] == \"continuous\":\n",
    "                normalized[name] = (tensor - self.scalers[name][\"mean\"]) / self.scalers[name][\"std\"]\n",
    "            else:\n",
    "                normalized[name] = tensor\n",
    "                \n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNetwork(torch.nn.Module):\n",
    "    \"\"\"Cross network for feature interactions\"\"\"\n",
    "    def __init__(self, input_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Cross layers\n",
    "        self.cross_layers = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_dim, input_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norms = torch.nn.ModuleList([\n",
    "            torch.nn.LayerNorm(input_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x0 = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Cross operation\n",
    "            cross = x0 * self.cross_layers[i](x)\n",
    "            # Residual connection\n",
    "            x = x + cross\n",
    "            # Layer normalization\n",
    "            x = self.layer_norms[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRankingModel(torch.nn.Module):\n",
    "    \"\"\"Ranking model with cross network and attention\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        num_categories: int,\n",
    "        embedding_dim: int = 64,\n",
    "        hidden_dim: int = 256,\n",
    "        num_heads: int = 4,\n",
    "        num_cross_layers: int = 3,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding tables\n",
    "        self.embedding_tables = torchrec.EmbeddingBagCollection(\n",
    "            tables=[\n",
    "                torchrec.EmbeddingBagConfig(\n",
    "                    name=\"user_embeddings\",\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=num_users,\n",
    "                    feature_names=[\"user_id\"],\n",
    "                ),\n",
    "                torchrec.EmbeddingBagConfig(\n",
    "                    name=\"item_embeddings\",\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=num_items,\n",
    "                    feature_names=[\"item_id\"],\n",
    "                ),\n",
    "                torchrec.EmbeddingBagConfig(\n",
    "                    name=\"category_embeddings\",\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=num_categories,\n",
    "                    feature_names=[\"category_id\"],\n",
    "                ),\n",
    "            ],\n",
    "            device=torch.device(\"meta\")\n",
    "        )\n",
    "        \n",
    "        # Dense feature processing\n",
    "        self.dense_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Cross network for feature interactions\n",
    "        self.cross_network = CrossNetwork(\n",
    "            input_dim=hidden_dim,\n",
    "            num_layers=num_cross_layers\n",
    "        )\n",
    "        \n",
    "        # Self-attention for feature refinement\n",
    "        self.self_attention = torch.nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Final ranking layers\n",
    "        self.ranking_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Embedding for position bias\n",
    "        self.position_embedding = torch.nn.Embedding(\n",
    "            num_embeddings=100,  # Max position\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        features: RankingFeatures,\n",
    "        return_embeddings: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Get embeddings\n",
    "        sparse_embeddings = self.embedding_tables(\n",
    "            KeyedJaggedTensor.from_lengths_sync(\n",
    "                keys=[\"user_id\", \"item_id\", \"category_id\"],\n",
    "                values=torch.cat([\n",
    "                    features.user_id,\n",
    "                    features.item_id,\n",
    "                    features.category_id\n",
    "                ]),\n",
    "                lengths=torch.ones(len(features.user_id) * 3)\n",
    "            )\n",
    "        ).to_dict()\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_embeddings = torch.cat([\n",
    "            sparse_embeddings[\"user_embeddings\"],\n",
    "            sparse_embeddings[\"item_embeddings\"],\n",
    "            sparse_embeddings[\"category_embeddings\"]\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Process dense features\n",
    "        dense_features = self.dense_network(combined_embeddings)\n",
    "        \n",
    "        # Apply cross network\n",
    "        cross_features = self.cross_network(dense_features)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attended_features, _ = self.self_attention(\n",
    "            cross_features.unsqueeze(1),\n",
    "            cross_features.unsqueeze(1),\n",
    "            cross_features.unsqueeze(1)\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        position_emb = self.position_embedding(features.position_ids)\n",
    "        \n",
    "        # Combine features\n",
    "        final_features = torch.cat([\n",
    "            attended_features,\n",
    "            position_emb\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Generate ranking scores\n",
    "        scores = self.ranking_layers(final_features)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return {\n",
    "                \"scores\": scores,\n",
    "                \"embeddings\": {\n",
    "                    \"user\": sparse_embeddings[\"user_embeddings\"],\n",
    "                    \"item\": sparse_embeddings[\"item_embeddings\"],\n",
    "                    \"cross\": cross_features,\n",
    "                    \"attention\": attended_features\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return {\"scores\": scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingLoss:\n",
    "    \"\"\"Combined ranking loss functions\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        lambda_pair: float = 1.0,\n",
    "        lambda_list: float = 1.0,\n",
    "        temperature: float = 1.0\n",
    "    ):\n",
    "        self.lambda_pair = lambda_pair\n",
    "        self.lambda_list = lambda_list\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def pointwise_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Binary cross entropy loss\"\"\"\n",
    "        return torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            scores.squeeze(),\n",
    "            labels.float()\n",
    "        )\n",
    "    \n",
    "    def pairwise_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Pairwise ranking loss with lambda weights\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        # Process each group (e.g., query) separately\n",
    "        for group_id in group_ids.unique():\n",
    "            mask = group_ids == group_id\n",
    "            group_scores = scores[mask]\n",
    "            group_labels = labels[mask]\n",
    "            \n",
    "            # Compute pairwise differences\n",
    "            score_diff = group_scores.unsqueeze(0) - group_scores.unsqueeze(1)\n",
    "            label_diff = group_labels.unsqueeze(0) - group_labels.unsqueeze(1)\n",
    "            \n",
    "            # Valid pairs have different labels\n",
    "            valid_pairs = label_diff != 0\n",
    "            \n",
    "            if valid_pairs.sum() > 0:\n",
    "                # Compute lambda weights (e.g., based on position or label difference)\n",
    "                lambda_weights = torch.abs(label_diff[valid_pairs])\n",
    "                \n",
    "                # Compute loss for valid pairs\n",
    "                pair_losses = torch.nn.functional.margin_ranking_loss(\n",
    "                    score_diff[valid_pairs],\n",
    "                    torch.zeros_like(score_diff[valid_pairs]),\n",
    "                    torch.sign(label_diff[valid_pairs]),\n",
    "                    reduction='none'\n",
    "                )\n",
    "                \n",
    "                # Weight the losses\n",
    "                losses.append((pair_losses * lambda_weights).mean())\n",
    "        \n",
    "        return torch.stack(losses).mean() if losses else torch.tensor(0.0).to(scores.device)\n",
    "    \n",
    "    def listwise_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"ListNet loss with temperature scaling\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for group_id in group_ids.unique():\n",
    "            mask = group_ids == group_id\n",
    "            group_scores = scores[mask]\n",
    "            group_labels = labels[mask]\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            scaled_scores = group_scores / self.temperature\n",
    "            \n",
    "            # Compute probabilities\n",
    "            score_probs = torch.nn.functional.softmax(scaled_scores, dim=0)\n",
    "            label_probs = torch.nn.functional.softmax(group_labels, dim=0)\n",
    "            \n",
    "            # Cross entropy between distributions\n",
    "            list_loss = -(label_probs * torch.log(score_probs + 1e-10)).sum()\n",
    "            losses.append(list_loss)\n",
    "        \n",
    "        return torch.stack(losses).mean()\n",
    "    \n",
    "    def combined_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        point_loss = self.pointwise_loss(scores, labels)\n",
    "        pair_loss = self.pairwise_loss(scores, labels, group_ids)\n",
    "        list_loss = self.listwise_loss(scores, labels, group_ids)\n",
    "        \n",
    "        total_loss = point_loss + self.lambda_pair * pair_loss + self.lambda_list * list_loss\n",
    "        \n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"point_loss\": point_loss,\n",
    "            \"pair_loss\": pair_loss,\n",
    "            \"list_loss\": list_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingTrainer:\n",
    "    \"\"\"Training infrastructure for advanced ranking model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AdvancedRankingModel,\n",
    "        loss_fn: RankingLoss,\n",
    "        learning_rate: float = 0.001,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        self.device = device\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=1000\n",
    "        )\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        features: RankingFeatures,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Move everything to device\n",
    "        features = RankingFeatures(*[f.to(self.device) for f in features])\n",
    "        labels = labels.to(self.device)\n",
    "        group_ids = group_ids.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(features, return_embeddings=True)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses = self.loss_fn.combined_loss(\n",
    "            outputs[\"scores\"],\n",
    "            labels,\n",
    "            group_ids\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        losses[\"total_loss\"].backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return {k: v.item() for k, v in losses.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ranking Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingMetrics:\n",
    "    \"\"\"Comprehensive ranking evaluation metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(scores: torch.Tensor, labels: torch.Tensor, k: int) -> float:\n",
    "        \"\"\"Normalized Discounted Cumulative Gain\"\"\"\n",
    "        device = scores.device\n",
    "        \n",
    "        # Sort by predicted scores\n",
    "        _, indices = scores.squeeze().sort(descending=True)\n",
    "        sorted_labels = labels[indices][:k]\n",
    "        \n",
    "        # Ideal ordering\n",
    "        _, ideal_indices = labels.sort(descending=True)\n",
    "        ideal_labels = labels[ideal_indices][:k]\n",
    "        \n",
    "        # Position discounts\n",
    "        positions = torch.arange(1, k + 1, dtype=torch.float, device=device)\n",
    "        discounts = 1.0 / torch.log2(positions + 1)\n",
    "        \n",
    "        # Calculate DCG and IDCG\n",
    "        dcg = (sorted_labels * discounts).sum()\n",
    "        idcg = (ideal_labels * discounts).sum()\n",
    "        \n",
    "        return (dcg / idcg).item() if idcg > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(scores: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "        \"\"\"Mean Reciprocal Rank\"\"\"\n",
    "        _, indices = scores.squeeze().sort(descending=True)\n",
    "        sorted_labels = labels[indices]\n",
    "        \n",
    "        # Find position of first relevant item\n",
    "        pos = (sorted_labels == 1).nonzero(as_tuple=True)[0]\n",
    "        if len(pos) > 0:\n",
    "            return (1.0 / (pos[0].item() + 1))\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_recall_at_k(\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        k: int\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"Precision and Recall at k\"\"\"\n",
    "        _, indices = scores.squeeze().sort(descending=True)\n",
    "        sorted_labels = labels[indices][:k]\n",
    "        \n",
    "        relevant_items = sorted_labels.sum().item()\n",
    "        total_relevant = labels.sum().item()\n",
    "        \n",
    "        precision = relevant_items / k\n",
    "        recall = relevant_items / total_relevant if total_relevant > 0 else 0.0\n",
    "        \n",
    "        return precision, recall\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_rankings(\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor,\n",
    "        k: int = 10\n",
    "    ) -> Dict[str, float]:\n",
    "        metrics = defaultdict(list)\n",
    "        \n",
    "        for group_id in group_ids.unique():\n",
    "            mask = group_ids == group_id\n",
    "            group_scores = scores[mask]\n",
    "            group_labels = labels[mask]\n",
    "            \n",
    "            metrics[\"ndcg\"].append(\n",
    "                RankingMetrics.ndcg_at_k(group_scores, group_labels, k)\n",
    "            )\n",
    "            metrics[\"mrr\"].append(\n",
    "                RankingMetrics.mean_reciprocal_rank(group_scores, group_labels)\n",
    "            )\n",
    "            precision, recall = RankingMetrics.precision_recall_at_k(\n",
    "                group_scores, group_labels, k\n",
    "            )\n",
    "            metrics[\"precision\"].append(precision)\n",
    "            metrics[\"recall\"].append(recall)\n",
    "        \n",
    "        return {\n",
    "            k: np.mean(v) for k, v in metrics.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDataGenerator:\n",
    "    \"\"\"Generate realistic ranking data\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        num_categories: int,\n",
    "        feature_dims: Dict[str, int],\n",
    "        max_list_length: int = 20\n",
    "    ):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_categories = num_categories\n",
    "        self.feature_dims = feature_dims\n",
    "        self.max_list_length = max_list_length\n",
    "        \n",
    "        # Generate synthetic user and item features\n",
    "        self.user_features = torch.randn(num_users, feature_dims[\"user\"])\n",
    "        self.item_features = torch.randn(num_items, feature_dims[\"item\"])\n",
    "        self.item_categories = torch.randint(0, num_categories, (num_items,))\n",
    "        \n",
    "        # Generate synthetic CTR data\n",
    "        self.item_base_ctr = torch.distributions.Beta(2, 10).sample((num_items,))\n",
    "    \n",
    "    def generate_batch(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        list_size: int\n",
    "    ) -> Tuple[RankingFeatures, torch.Tensor, torch.Tensor]:\n",
    "        # Sample users and items\n",
    "        user_ids = torch.randint(0, self.num_users, (batch_size,))\n",
    "        query_ids = torch.arange(batch_size).repeat_interleave(list_size)\n",
    "        \n",
    "        # For each user, sample items\n",
    "        item_ids = torch.randint(\n",
    "            0, self.num_items,\n",
    "            (batch_size * list_size,)\n",
    "        )\n",
    "        \n",
    "        # Get features\n",
    "        features = RankingFeatures(\n",
    "            user_features=self.user_features[user_ids].repeat_interleave(list_size, dim=0),\n",
    "            item_features=self.item_features[item_ids],\n",
    "            context_features=torch.randn(batch_size * list_size, self.feature_dims[\"context\"]),\n",
    "            user_id=user_ids.repeat_interleave(list_size),\n",
    "            item_id=item_ids,\n",
    "            category_id=self.item_categories[item_ids],\n",
    "            historical_ctr=self.item_base_ctr[item_ids],\n",
    "            user_item_similarity=torch.randn(batch_size * list_size),\n",
    "            history_length=torch.randint(1, 50, (batch_size * list_size,)),\n",
    "            position_ids=torch.arange(list_size).repeat(batch_size)\n",
    "        )\n",
    "        \n",
    "        # Generate labels (click probability based on features)\n",
    "        labels = torch.sigmoid(\n",
    "            0.1 * features.user_item_similarity +\n",
    "            0.2 * features.historical_ctr +\n",
    "            -0.1 * features.position_ids.float()\n",
    "        )\n",
    "        \n",
    "        # Convert to binary labels\n",
    "        binary_labels = torch.bernoulli(labels)\n",
    "        \n",
    "        return features, binary_labels, query_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ranking_model():\n",
    "    # Initialize components\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    feature_dims = {\n",
    "        \"user\": 10,\n",
    "        \"item\": 20,\n",
    "        \"context\": 5\n",
    "    }\n",
    "    \n",
    "    model = AdvancedRankingModel(\n",
    "        num_users=100_000,\n",
    "        num_items=1_000_000,\n",
    "        num_categories=1000,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=256\n",
    "    )\n",
    "    \n",
    "    loss_fn = RankingLoss(\n",
    "        lambda_pair=0.5,\n",
    "        lambda_list=0.5,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    trainer = RankingTrainer(model, loss_fn, device=device)\n",
    "    data_gen = RankingDataGenerator(\n",
    "        num_users=100_000,\n",
    "        num_items=1_000_000,\n",
    "        num_categories=1000,\n",
    "        feature_dims=feature_dims\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "    list_size = 10\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_losses = defaultdict(list)\n",
    "        \n",
    "        for batch in range(100):  # 100 batches per epoch\n",
    "            features, labels, group_ids = data_gen.generate_batch(\n",
    "                batch_size, list_size\n",
    "            )\n",
    "            \n",
    "            losses = trainer.train_step(features, labels, group_ids)\n",
    "            \n",
    "            for k, v in losses.items():\n",
    "                epoch_losses[k].append(v)\n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print(f\"Batch {batch}, Loss: {losses['total_loss']:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # 10 eval batches\n",
    "                features, labels, group_ids = data_gen.generate_batch(\n",
    "                    batch_size=100,  # Larger eval batch\n",
    "                    list_size=list_size\n",
    "                )\n",
    "                \n",
    "                features = RankingFeatures(\n",
    "                    *[f.to(device) for f in features]\n",
    "                )\n",
    "                \n",
    "                scores = model(features)[\"scores\"]\n",
    "                metrics = RankingMetrics.evaluate_rankings(\n",
    "                    scores, labels.to(device), group_ids.to(device)\n",
    "                )\n",
    "                \n",
    "                for k, v in metrics.items():\n",
    "                    eval_metrics[k].append(v)\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        for k, v in eval_metrics.items():\n",
    "            print(f\"{k}: {np.mean(v):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_ranking_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
