{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ranking System with TorchRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrec\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, NamedTuple\n",
    "from dataclasses import dataclass\n",
    "from torchrec.sparse.jagged_tensor import KeyedJaggedTensor\n",
    "from utils.data_generators import TorchRecDataGenerator\n",
    "from utils.debugging import TorchRecDebugger\n",
    "from utils.benchmark import TorchRecBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RankingFeatures:\n",
    "    \"\"\"Complex feature set for ranking\"\"\"\n",
    "    # Dense features\n",
    "    user_features: torch.Tensor      # age, gender, etc.\n",
    "    item_features: torch.Tensor      # price, age, etc.\n",
    "    context_features: torch.Tensor   # time, device, etc.\n",
    "    \n",
    "    # Sparse features (IDs)\n",
    "    user_id: torch.Tensor\n",
    "    item_id: torch.Tensor\n",
    "    category_id: torch.Tensor\n",
    "    \n",
    "    # Interaction features\n",
    "    historical_ctr: torch.Tensor    # Historical click-through rate\n",
    "    user_item_similarity: torch.Tensor\n",
    "    \n",
    "    # Sequence features\n",
    "    history_length: torch.Tensor\n",
    "    position_ids: torch.Tensor\n",
    "\n",
    "class FeatureProcessor:\n",
    "    \"\"\"Process and normalize features\"\"\"\n",
    "    def __init__(self, feature_config: Dict[str, Dict]):\n",
    "        self.feature_config = feature_config\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def fit(self, features: Dict[str, torch.Tensor]):\n",
    "        \"\"\"Compute normalization parameters\"\"\"\n",
    "        for name, config in self.feature_config.items():\n",
    "            if config[\"type\"] == \"continuous\":\n",
    "                self.scalers[name] = {\n",
    "                    \"mean\": features[name].mean(),\n",
    "                    \"std\": features[name].std()\n",
    "                }\n",
    "    \n",
    "    def transform(self, features: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Normalize features\"\"\"\n",
    "        normalized = {}\n",
    "        for name, tensor in features.items():\n",
    "            config = self.feature_config[name]\n",
    "            \n",
    "            if config[\"type\"] == \"continuous\":\n",
    "                normalized[name] = (tensor - self.scalers[name][\"mean\"]) / self.scalers[name][\"std\"]\n",
    "            else:\n",
    "                normalized[name] = tensor\n",
    "                \n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNetwork(torch.nn.Module):\n",
    "    \"\"\"Cross network for feature interactions\"\"\"\n",
    "    def __init__(self, input_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Cross layers\n",
    "        self.cross_layers = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_dim, input_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norms = torch.nn.ModuleList([\n",
    "            torch.nn.LayerNorm(input_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x0 = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Cross operation\n",
    "            cross = x0 * self.cross_layers[i](x)\n",
    "            # Residual connection\n",
    "            x = x + cross\n",
    "            # Layer normalization\n",
    "            x = self.layer_norms[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRankingModel(torch.nn.Module):\n",
    "    \"\"\"Ranking model with cross network and attention\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        num_categories: int,\n",
    "        embedding_dim: int = 64,\n",
    "        hidden_dim: int = 256,\n",
    "        num_heads: int = 4,\n",
    "        num_cross_layers: int = 3,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding tables\n",
    "        self.embedding_tables = torchrec.EmbeddingBagCollection(\n",
    "            tables=[\n",
    "                torchrec.EmbeddingBagConfig(\n",
    "                    name=\"user_embeddings\",\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=num_users,\n",
    "                    feature_names=[\"user_id\"],\n",
    "                ),\n",
    "                torchrec.EmbeddingBagConfig(\n",
    "                    name=\"item_embeddings\",\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=num_items,\n",
    "                    feature_names=[\"item_id\"],\n",
    "                ),\n",
    "                torchrec.EmbeddingBagConfig(\n",
    "                    name=\"category_embeddings\",\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=num_categories,\n",
    "                    feature_names=[\"category_id\"],\n",
    "                ),\n",
    "            ],\n",
    "            device=torch.device(\"meta\")\n",
    "        )\n",
    "        \n",
    "        # Dense feature processing\n",
    "        self.dense_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Cross network for feature interactions\n",
    "        self.cross_network = CrossNetwork(\n",
    "            input_dim=hidden_dim,\n",
    "            num_layers=num_cross_layers\n",
    "        )\n",
    "        \n",
    "        # Self-attention for feature refinement\n",
    "        self.self_attention = torch.nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Final ranking layers\n",
    "        self.ranking_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Embedding for position bias\n",
    "        self.position_embedding = torch.nn.Embedding(\n",
    "            num_embeddings=100,  # Max position\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        features: RankingFeatures,\n",
    "        return_embeddings: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Get embeddings\n",
    "        sparse_embeddings = self.embedding_tables(\n",
    "            KeyedJaggedTensor.from_lengths_sync(\n",
    "                keys=[\"user_id\", \"item_id\", \"category_id\"],\n",
    "                values=torch.cat([\n",
    "                    features.user_id,\n",
    "                    features.item_id,\n",
    "                    features.category_id\n",
    "                ]),\n",
    "                lengths=torch.ones(len(features.user_id) * 3)\n",
    "            )\n",
    "        ).to_dict()\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_embeddings = torch.cat([\n",
    "            sparse_embeddings[\"user_embeddings\"],\n",
    "            sparse_embeddings[\"item_embeddings\"],\n",
    "            sparse_embeddings[\"category_embeddings\"]\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Process dense features\n",
    "        dense_features = self.dense_network(combined_embeddings)\n",
    "        \n",
    "        # Apply cross network\n",
    "        cross_features = self.cross_network(dense_features)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attended_features, _ = self.self_attention(\n",
    "            cross_features.unsqueeze(1),\n",
    "            cross_features.unsqueeze(1),\n",
    "            cross_features.unsqueeze(1)\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        position_emb = self.position_embedding(features.position_ids)\n",
    "        \n",
    "        # Combine features\n",
    "        final_features = torch.cat([\n",
    "            attended_features,\n",
    "            position_emb\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Generate ranking scores\n",
    "        scores = self.ranking_layers(final_features)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return {\n",
    "                \"scores\": scores,\n",
    "                \"embeddings\": {\n",
    "                    \"user\": sparse_embeddings[\"user_embeddings\"],\n",
    "                    \"item\": sparse_embeddings[\"item_embeddings\"],\n",
    "                    \"cross\": cross_features,\n",
    "                    \"attention\": attended_features\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return {\"scores\": scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingLoss:\n",
    "    \"\"\"Combined ranking loss functions\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        lambda_pair: float = 1.0,\n",
    "        lambda_list: float = 1.0,\n",
    "        temperature: float = 1.0\n",
    "    ):\n",
    "        self.lambda_pair = lambda_pair\n",
    "        self.lambda_list = lambda_list\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def pointwise_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Binary cross entropy loss\"\"\"\n",
    "        return torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            scores.squeeze(),\n",
    "            labels.float()\n",
    "        )\n",
    "    \n",
    "    def pairwise_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Pairwise ranking loss with lambda weights\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        # Process each group (e.g., query) separately\n",
    "        for group_id in group_ids.unique():\n",
    "            mask = group_ids == group_id\n",
    "            group_scores = scores[mask]\n",
    "            group_labels = labels[mask]\n",
    "            \n",
    "            # Compute pairwise differences\n",
    "            score_diff = group_scores.unsqueeze(0) - group_scores.unsqueeze(1)\n",
    "            label_diff = group_labels.unsqueeze(0) - group_labels.unsqueeze(1)\n",
    "            \n",
    "            # Valid pairs have different labels\n",
    "            valid_pairs = label_diff != 0\n",
    "            \n",
    "            if valid_pairs.sum() > 0:\n",
    "                # Compute lambda weights (e.g., based on position or label difference)\n",
    "                lambda_weights = torch.abs(label_diff[valid_pairs])\n",
    "                \n",
    "                # Compute loss for valid pairs\n",
    "                pair_losses = torch.nn.functional.margin_ranking_loss(\n",
    "                    score_diff[valid_pairs],\n",
    "                    torch.zeros_like(score_diff[valid_pairs]),\n",
    "                    torch.sign(label_diff[valid_pairs]),\n",
    "                    reduction='none'\n",
    "                )\n",
    "                \n",
    "                # Weight the losses\n",
    "                losses.append((pair_losses * lambda_weights).mean())\n",
    "        \n",
    "        return torch.stack(losses).mean() if losses else torch.tensor(0.0).to(scores.device)\n",
    "    \n",
    "    def listwise_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"ListNet loss with temperature scaling\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for group_id in group_ids.unique():\n",
    "            mask = group_ids == group_id\n",
    "            group_scores = scores[mask]\n",
    "            group_labels = labels[mask]\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            scaled_scores = group_scores / self.temperature\n",
    "            \n",
    "            # Compute probabilities\n",
    "            score_probs = torch.nn.functional.softmax(scaled_scores, dim=0)\n",
    "            label_probs = torch.nn.functional.softmax(group_labels, dim=0)\n",
    "            \n",
    "            # Cross entropy between distributions\n",
    "            list_loss = -(label_probs * torch.log(score_probs + 1e-10)).sum()\n",
    "            losses.append(list_loss)\n",
    "        \n",
    "        return torch.stack(losses).mean()\n",
    "    \n",
    "    def combined_loss(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        group_ids: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        point_loss = self.pointwise_loss(scores, labels)\n",
    "        pair_loss = self.pairwise_loss(scores, labels, group_ids)\n",
    "        list_loss = self.listwise_loss(scores, labels, group_ids)\n",
    "        \n",
    "        total_loss = point_loss + self.lambda_pair * pair_loss + self.lambda_list * list_loss\n",
    "        \n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"point_loss\": point_loss,\n",
    "            \"pair_loss\": pair_loss,\n",
    "            \"list_loss\": list_loss\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
